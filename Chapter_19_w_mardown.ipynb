{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19 -  **Chapter name here**\n",
    "\n",
    "Add summary\n",
    "\n",
    "Add intro.\n",
    "\n",
    "## 1. ORGANISE YOUR WORKSPACE\n",
    "\n",
    "Create a folder in which you can store all your experiments. After a lot of testing it will be easy to feel lost. Give your experiment a name and create a folder in a chosen directory with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "experiment_name = \"example\"\n",
    "if not os.path.exists(\"./results/\" + experiment_name + \"/\"):\n",
    "    os.makedirs(\"./results/\" + experiment_name + \"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DEFINE YOUR PROBLEM \n",
    "\n",
    "The first step in your machine learning pipeline is to define your problem. To do this you will need to first load your data and then define feature set X and the corresponding labels y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loading the data\n",
    "\n",
    "In this example, we will use tabular data with features as columns and rows as participants. The data are saved as a csv file. We will use the library pandas to load and explore the data. The first thing we need to do is import the pandas library. To make the code simpler, it is common to import it as pd. This way, we simply type \"pd\" everytime we want to call the pandas library. From pandas, we use the function read_csv() to load the csv file containing our data. This will transform the data into an object called dataframe.\n",
    "#Fix: add link to dummy data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./Chapter_19_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by seeing the first six rows of the data. Selecting subsections (\"slicing\") of a dataframe using pandas is straighforward. There are different ways to do this. Here, we use loc to select the first five rows (note that the first column is indexed 0 and the last column is not included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the features names at the top and the data for the first six participants. Features include the ID, age and gender, as well as the gray matter volume for several brain regions. We can see that there is at least one value missing. We will deal with this later. \n",
    "\n",
    "It may be useful to know all the features that are avaiable in the dataset. To do this, we simply ask for the names of the columns of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what features we are dealing with, let's check the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features =\", data.shape[1])\n",
    "print(\"Number of participants =\", data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Define your feature set and target\n",
    "\n",
    "The target is a categorical variable that determines whether a participant belongs to the HC and SZ group. Therefore, we will refer to the target variable as labels. This variable corresponds to the column named \"label\" in the dataframe. To create a new variable y with these data, we simply retrieve this column from the dataframe \"data\". Defining the features in the same way would be impractical, since the are too many to be named individually. Instead, we can select a range of columns based on their location. We already know that there are 173 features in the original data. However, not all are brain region volumes; the relevant features for our model start from the 5th column. Therefore, we select the columns of the dataframe accordingly by retrieving a selection of the data containing all rows from the 5th column onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = data[\"label\"]\n",
    "X = data[data.columns[4:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PREPARE YOUR DATA\n",
    "\n",
    "In this step we want to get our data ready before we start analyse it. This can involve different analyses depending on the nature of your problem and type of features. \n",
    "\n",
    "In this example, we will check the data for the presence of:\n",
    " - Data imbalance with respect to the labels\n",
    " - Confounding variables\n",
    " - Missing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Class imbalance\n",
    "\n",
    "First, let's check the number of total participants, features and number of participants in each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with schizophrenia (SZ) =\", len(y[y == 0]))\n",
    "print(\"Number of Healthy Control (HC) =\", len(y[y == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can see that there are 740 participants in total, 368 patients and 372 controls. There does not seem to be a large imbalance between classes. However, they are not perfectly matched. One option would be to downsample the HC to match the SZ group. However, this would mean loosing some data. Since the imbalance is not too large, we will use balanced accuracy as our metric of choice as well as stratified CV to ensure the same proportion SZ/HC across the CV iterations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Confounding variables\n",
    "\n",
    "Next, let's check for balance of some obvious confounding variables: gender and age. First, let's see the distribution of gender between the two classes. This time, we will use some plots from the seaborn library. Plotting data using seaborn is straighforward. For more information see https://seaborn.pydata.org. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ax = sns.countplot(x=\"label\", hue=\"Gender\", data=data, palette=\"Blues_d\")\n",
    "\n",
    "plt.legend([\"Male\", \"Female\"])\n",
    "plt.xticks([0,1], [\"SZ\", \"HC\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a fairly similar distribution of males and females in the two classes. Therefore, we will not consider gender as a significant confounder. \n",
    "- do not make assumptions by visual checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "tab = pd.crosstab(data[\"Gender\"], data[\"label\"])\n",
    "print(tab)\n",
    "\n",
    "chi2, p_value, _, _ = stats.chi2_contingency(tab, correction=False)\n",
    "print(chi2)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check for any imbalance with respect to age by retrieving the mean age and standard deviation for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (data.groupby(\"label\")[\"Age\"].mean()).round(1)\n",
    "sd = (data.groupby(\"label\")[\"Age\"].std()).round(1)\n",
    "print(\"\")\n",
    "print(\"Mean age for HC =\", mean[0],\"±\",sd[0])\n",
    "print(\"Mean age for SZ =\", mean[1],\"±\",sd[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sz = data[data['label']==0]['Age']\n",
    "age_hc = data[data['label']==1]['Age']\n",
    "\n",
    "statistic, p_value = stats.ttest_ind(age_sz, age_hc)\n",
    "print(statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the output above, the mean and standard-deviations for age are indeed very similar between the SZ and HC groups. For this reason, we will not consider age as a significant confounder in this example.\n",
    "\n",
    "Had there been a significant imbalance in gender or age (or any other relevant variable) there were several potential options to choose from. Perhaps the most obvious and simple one would be to match the groups with respect to these variables. Another popular option would be to regress out the confounding variables from each feature and using the residuals as the new features. For more information on confounding variables in neuroimaging and machine learning learning see Rao et al., 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Missing data\n",
    "\n",
    "Many machine learning algorithms do not support data with missing values. Therefore it is important to check if there are any missing values in our data. There are many different ways to do this. Here we will build our own function to loop through each the column in the dataframe data (note data we are also including gender and age) and get the feature name and Id for the corresponding missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nan(dataset):\n",
    "    nan_total = dataset.isnull().sum().sum() \n",
    "    if nan_total > 0:\n",
    "        for column in dataset:          \n",
    "            #Find Ids with nan - THIS IS PROBABLY OVERLY COMPLICATED (all I want here is to get the Ids of where the nans are so I can print them later on)\n",
    "            nan = dataset[column].isnull()\n",
    "            dataset[\"nan\"] = nan \n",
    "            ids = []\n",
    "            for i in dataset[\"nan\"]:\n",
    "                if i == True:\n",
    "                    id_nan = dataset.loc[dataset[\"nan\"] == True, 'ID']\n",
    "                    ids.append(id_nan)               \n",
    "            #Calculate total number of nan for each feature and Id\n",
    "            nan_sum = nan.sum()          \n",
    "            if nan_sum > 0:\n",
    "                print(\"Found\", nan_sum, \"missing value(s) for\", column, \"for Id(s):\", *ids[0])             \n",
    "        #dataset = dataset.drop(columns=[\"nan\"])\n",
    "    else:\n",
    "        print(\"There are no missing data in this dataset!\")\n",
    "        print(\"\")\n",
    "detect_nan(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can see that there are 12 missing values for the feature Left Lateral Ventricle. There are several options to go from here with different degrees of complexity (see Chapter x for a more in-depth description). In this example, we will impute the data #missing for Left Lateral Ventricle. Recall that missing data should be imputed inside the cross validation (CV). Therefore, this step will be implemented this later.\n",
    "\n",
    "FIX: At the moment thre is no missing data. I will remove some data to make it more interesting. This code works but I think it is probably overly complicated...I have to add and remove a column to the dataframe with booleans \"nan\" to identify NAN and it does not seem so intuitive. If you could find a cool way to simplify it, it would be great! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FEATURE ENGINEERING\n",
    "\n",
    "In this step, we want to make all the necessary transformations to our data that can help us build a good model. As described in Chapter 2, this can involve different procedures depending on the nature of the data. In this example, we want to use neuroanatomical data to classify SZ and HC.\n",
    "\n",
    "### 4.1. Feature extraction\n",
    "\n",
    "This first step involves extracting brain morphometric information from the raw MRI images. Luckily, this step has already been done for us. The regional grey matter volumes that make up our data X have been extracted with FreeSurfer (REF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Cross-validation\n",
    "\n",
    "Before we move on to apply any transformations to our feature set X, we need to split the data into train and test sets. Recall that this is a critical step to ensure independence between the training and test sets. There are different ways in which to do this. In this example, we will use stratified 10-fold cross-validation (CV). See Chapter 2 for an overview and rational of the most commonly used types of CV. \n",
    "\n",
    "We first transform the dataframe X into a 2D array of the same shape using the library numpy. For the purpose of this exercise, you can think of this as a 2x2 matrix. This will make it easier later on, as some of the functions will require the data to be in this format. \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.asarray(X.values, dtype='float32')\n",
    "y = np.asarray(y.values, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the parameters of our stratified CV scheme using the function StratifiedKFold from the sklearn library, and assign it to the variable skf.\n",
    "\n",
    "Notice the hyperparamter random_state in the specification of the CV assigned to the variable skf. This hyperparamter allows us to control the element of randomness intrinsic to splitting the total data into train and test sets. In this example, our data comprises of 740 participants in total. In the code above, we have instructed the model to split it into 10 groups (whilst maintaining the SZ/HC ratio similar throughout the CV iterations). Now, there are multiple solutions to this task! Not setting this hyperparameter to a specific value means that, every time you run your code, the participants assigned to each group will differ! Consequently, your results will, very likely, differ as well*. This is something we would like to avoid, at least while we build upon our model to improve it, as we want to be able to reproduce the same results for comparison between different models.\n",
    "\n",
    "*Importantly to brain disorders research, this is specially true for small sample sizes. For an interesting discussion on the relation between sample size, cross-validation and performance see Varoquax et al. 2018 and Nieuwenhuis et al. 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 10\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that the CV is defined, we can loop over each one of the 10 train/test set iterations. At each iteration, we will transform and fit the machine learning algorithm to the training set; and apply the same data transformations and test the algorithm in the test set. \n",
    "\n",
    "We can implement the above using a for loop to iterate over the 10 i_folds. At each i_fold, we will have four new variables:\n",
    " - X_train and y_train: training set and corresponding labels\n",
    " - X_test and y_test: test set and corresponding labels\n",
    "\n",
    "Because we will be training and testing the machine learning algorithm one iteration at a time, we will create some empty variables where we can store important information from each iteration. In the code below we create four empty arrays of shape 10, one for each performance metric: balanced accuracy (bac), sensitivity (sens), specificity (spec) and error rate. Each array will be populated with each metric from each CV iteration. We also created an empty list coefficients. This is where we will store the weights (coefficient or \"importance\") from each feature across the CV iterations. Once the CV is finished, we can then calculate the average weight of each feature for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_bac = np.zeros((n_folds,))\n",
    "cv_test_sens = np.zeros((n_folds,))\n",
    "cv_test_spec = np.zeros((n_folds,))\n",
    "cv_error_rate = np.zeros((n_folds,))\n",
    "coefficients = []\n",
    "\n",
    "for i_fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many participants there are in the train and test sets in each iteration of the CV. We can do this by simply asking for the length of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\")\n",
    "    print(\"k-fold: \", i_fold + 1)\n",
    "    print(\"N training set:\", len(y_train))\n",
    "    print(\"N test set:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Data imputation\n",
    "\n",
    "As we saw before, it was decided to use imputation to fill in the missing data for the feature Left Lateral Ventricle. There are several methods to implement this. In this example, we use the function Imputer from sklearn to 1) compute the mean value for this feature in the training set, and 2) replace all the missing values in the training and 3) test sets with the same mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 1)\n",
    "imputer = imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Feature scaling/normalization\n",
    "\n",
    "Before making any transformation to our data, we want to make sure that the fact that different regions of the brain have different ranges will not affect our model. For example, the hippocampus will tend to be larger than the thalamus. If we keep the original values, the algorithm may mistakenly conclude that the hippocampus will be less important than region the thalamus. - IS THIS CORRECT?\n",
    "\n",
    "There are several possible solutions to avoid this and other related issues. In this example, we will transform the data in such a way that the distribution of each feature will resemble a normal distribution (e.g. mean=0 and standard-deviation=1). Each new normalised value z is calculated by taking each data point xi, subtracting the mean x_ and then dividing it by the standard-deviation sd of the same feature:\n",
    "\n",
    "zxi = (x_featureA - xi) / sdfeatureA\n",
    "\n",
    "The code below normalises each feature independently using the function StandardScaler from sklearn.\n",
    "\n",
    "First, we fit the function StandardScaler to the train set and store the parameters - x_ and sd - in the variable scaler. Then, we transform both the train and test set using the formula above with the stored parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_n = scaler.transform(X_train)\n",
    "    X_test_n = scaler.transform(X_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what effect this had on the data. Let's select three features at random from the training set - columns 2, 65, and 100 - and plot each distribution before and after normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2_before, feature_65_before, feature_100_before  = X_train[:,2], X_train[:,65], X_train[:,100]\n",
    "feature_2_after, feature_65_after, feature_100_after = X_train_n[:,2], X_train_n[:,65], X_train_n[:,100]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "p1_before=sns.kdeplot(feature_2_before, color=\"r\", label=(\"Feature 2 before normalization\"))\n",
    "p1_before=sns.kdeplot(feature_65_before, color=\"b\", label=(\"Feature 65 before normalization\"))\n",
    "p1_before=sns.kdeplot(feature_100_before, color=\"g\", label=(\"Feature 100 before normalization\"))\n",
    "p1_before.set_title('Distribution before normalization')\n",
    "plt.show()\n",
    "\n",
    "p1_after=sns.kdeplot(feature_2_after, color=\"r\", label=(\"Feature 2 after normalization\"))\n",
    "p1_after=sns.kdeplot(feature_65_after, color=\"b\", label=(\"Feature 65 after normalization\"))\n",
    "p1_after=sns.kdeplot(feature_100_after, color=\"g\", label=(\"Feature 100 after normalization\"))\n",
    "p1_before.set_title('Distribution after normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot shows three very distinct distributions. On the other hand, the second plot shows almost overlapping distributions!\n",
    "\n",
    "FIX: !!TOO MANY LINES...NOT VERY INTUITIVE!! Is it useful at all?? Maybe a bit anoying having this at every fold??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Feature selection\n",
    "\n",
    "Our initial feature set contains 169 features. It is reasonable to assume that some features (i.e. grey matter volume of some brain regions) may be more useful than others for distinguishing SZ patients from controls. Removing less relevant features will speed up the training process and may even improve prediction.\n",
    "\n",
    "This can be done by adding a step known as feature selection to our model. You may recall from Chapter 2 that there are several different ways in which we can implement this step. In this example, we will use the popular recursive feature elimination (RFE) method. In principle, this is how RFE works:\n",
    "  1. The model is fit to the training data\n",
    "  2. The coefficient of each feature (or feature importance) is retrieved\n",
    "  3. Features are ranked according to their coefficients\n",
    "  4. The feature (or features, depending on the hyperparamenters chosen) with least importance is dropped\n",
    "  5. The model is fit to the remaining features\n",
    "  6. Steps 2-5 are repeated until the desired number of features has been reached\n",
    "\n",
    "From the description above, it follows that the implementation of RFE relies on two hyperparameters: a) step - the number of features to remove in step 4, b) n_features_to_select - the final number of desired features. For simplicity, we will keep these hyperparameters with their default values: step = 1 and n_features_to_select = half of the features = 85.   \n",
    "\n",
    "Recall that feature selection should be done within the CV. Thefore, RFE will be be implementated in the model training section. \n",
    "\n",
    "FIX: add RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MODEL TRAINING\n",
    "\n",
    "### 5.1. Balanced accuracy\n",
    "\n",
    "As part of the model training process, we need to specify a performance metric. Our data is not that unbalanced, however, we will still use balanced accuracy since this is more likely to be use (note that for perfectly balanced dataset, accuracy and balanced accuracy should be the same). Unfortunately, the long list of performance metrics available form sklearn does not yet include balanced accuracy. Fortunately however, it does allow us to build a custom metric. This means we can create a function grid_scorer that calculates balanced accuracy which we can then pass through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.metrics import make_scorer\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    def balanced_accuracy_score(actual, prediction):\n",
    "        cm = confusion_matrix(actual, prediction)\n",
    "        bac = np.sum(np.true_divide(np.diagonal(cm), np.sum(cm, axis=1))) / cm.shape[1]\n",
    "        return bac\n",
    "    grid_scorer = make_scorer(balanced_accuracy_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Machine learning algorithm and hyper-parameter optimization\n",
    "\n",
    "Now that we have the performance metric defined, we can specify our machine learning algorithm. In this example, we will use the popular support vector machine (SVM) as implemented by the sklearn library. You may remember from Chapter 6 that SVM allows the use of different kernels to best separate classes. Here, we will use the default linear kernel for simplicity. You can find more information about different kernels at https://scikit-learn.org/stable/modules/svm.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.svm import LinearSVC\n",
    "\n",
    "    clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, SVM relies on a hyperparameter C that regulates how much we want to avoid misclassifying each training example (see Chapter 6). The ideal method for choosing the value of C is by letting the model try several values and selecting the one with the best performance. This should be done via an additional CV inside the already defined CV, thus creating a nested CV where different values of C are fitted to the training set and tested in the validation set; the value of C with best performance is then used to fit the model to the training set as defined by the outer CV (see Chapter 2 for more details).\n",
    "\n",
    "Fortunately, sklearn has a set of useful tools to implement this. Here, we will use GridSearch, a popular choice in the brain disorders literature. You can find more about GridSearch and other methods for hyperparameter optimisation at https://scikit-learn.org/stable/modules/grid_search.html.\n",
    "\n",
    "To implement GridSearch, we first need to provide a range of possible values for C; this is our search parameter space. Next, we specify the parameters for the GridSearch. We will use stratified kfold again with 10 iterations, as with the outer CV previously defined. The final model design can be seen in Figure 1. - Thinking of adding a figure to show the final workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    #Hyperparameter seach space\n",
    "    param_grid = [\n",
    "        {'C': [2e-6, 2e-5, 2e-4, 2e-3, 2e-2, 2e-1, 2e0, 2e1, 2e2, 2e3]},\n",
    "    ]\n",
    "    \n",
    "    #Gridsearch\n",
    "    internal_cv = StratifiedKFold(n_splits=10)\n",
    "    grid_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=internal_cv, scoring=grid_scorer, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit our SVM model to the training data. We do this by applying the fit command to the features and labels from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    grid_result = grid_clf.fit(X_train_n, y_train) #FIX: this runs but it is giving me a \"Convergence Warning\" and I don't know why.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how GridSearch works: we can see the performance for the different values of C in the validation set within the inner CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of C that yields the best performance in the validation set is shown at the top. We then define a second clasifier clf2, where C takes the best perfomring value and fit it to the training data as defined by the outer CV. Finally, we use this model to make predictions in the test set; these are stored in y_predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf2 = LinearSVC(C=grid_result.best_params_[\"C\"])\n",
    "    clf2.fit(X_train_n, y_train)\n",
    "    y_predicted = clf2.predict(X_test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the predicted labels, we can now estimate the performance in the test set. First, we compute the confusion matrix. From here, we estimate balanced accuracy, sensitivity, specificity and error rate. There are plenty more metrics to choose from in sklearn. For a comprehensive list see https://scikit-learn.org/stable/modules/model_evaluation.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Confusion matrix\")\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "    print(\"\")\n",
    "\n",
    "    test_bac = np.sum(np.true_divide(np.diagonal(cm), np.sum(cm, axis=1))) / cm.shape[1]\n",
    "    test_sens = np.true_divide(cm[1, 1], np.sum(cm[1, :]))\n",
    "    test_spec = np.true_divide(cm[0, 0], np.sum(cm[0, :]))\n",
    "    error_rate = np.true_divide(cm[0, 1] + cm[1, 0], np.sum(np.sum(cm)))\n",
    "\n",
    "    print(\"Balanced acc: %.4f \" % (test_bac))\n",
    "    print(\"Sensitivity: %.4f \" % (test_sens))\n",
    "    print(\"Specificity: %.4f \" % (test_spec))\n",
    "    print(\"Error Rate: %.4f \" % (error_rate))\n",
    "\n",
    "    cv_test_bac[i_fold - 1] = test_bac\n",
    "    cv_test_sens[i_fold - 1] = test_sens\n",
    "    cv_test_spec[i_fold - 1] = test_spec\n",
    "    cv_error_rate[i_fold - 1] = error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Save model's coefficients\n",
    "\n",
    "In addition to model performance, we are also interested in knowing which features are driving the model's predictions. Sklearn has an in-built tool coef_ that we can apply to our SVM model to extract this information. We will store the coefficients for each feature for a particular CV iteration in coefficients_fold. Next, we append these values to the empty list coefficients we have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    coefficients_fold = clf2.coef_\n",
    "    coefficients.append(coefficients_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Save the model's predictions\n",
    "\n",
    "Let's save the true labels, the predicted labels and the trained model for each CV iteration. This informtion may come in handy later on for some additional analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import csv\n",
    "    import pickle\n",
    "    \n",
    "    if i_fold == 0:\n",
    "        file_predictions = open(\"./results/\" + experiment_name + \"/error_analysis.csv\", 'w')\n",
    "        wr = csv.writer(file_predictions)\n",
    "        wr.writerow(['INDEX', 'TRUE LABEL', 'PREDICTED'])\n",
    "    else:\n",
    "        file_predictions = open(\"./results/\" + experiment_name + \"/error_analysis.csv\", 'a')\n",
    "        wr = csv.writer(file_predictions)\n",
    "    for j, fname in enumerate(test_index):\n",
    "        wr.writerow([(str(fname)), str(y_test[j]), str(y_predicted[j])])\n",
    "    wr.writerow(['-', '-', '-'])\n",
    "    file_predictions.close()\n",
    "    \n",
    "    f = open(\"./results/\"+experiment_name+\"/clf_\"+str(i_fold)+\".pkl\", 'wb')\n",
    "    pickle.dump(clf, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ASSESSING PERFORMANCE\n",
    "\n",
    "Once the 10 iterations of the CV are finished, we calculate the average of each chosen metric across all iterations. The result will be the final overall performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(\"Cross-validation Balanced acc: %.4f +- %.4f\" % (cv_test_bac.mean(), cv_test_bac.std()))\n",
    "print(\"Cross-validation Sensitivity: %.4f +- %.4f\" % (cv_test_sens.mean(), cv_test_sens.std()))\n",
    "print(\"Cross-validation Specificity: %.4f +- %.4f\" % (cv_test_spec.mean(), cv_test_spec.std()))\n",
    "print(\"Cross-validation Error Rate: %.4f +- %.4f\" % (cv_error_rate.mean(), cv_error_rate.std()))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to save your main results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"./results/\" + experiment_name + \"/final_BAC.pkl\", 'wb')\n",
    "pickle.dump(cv_test_bac, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. POST-HOC ANALYSIS\n",
    "\n",
    "Once we have our final model, we can several additional analysis. Here, we will run the following analyses:\n",
    " - Test balanced accuracy for statistical significance via permutation testing \n",
    " - Identify the features that contributed the most for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Best features\n",
    "\n",
    "\n",
    "EXPLAIN CODE HERE IF THIS IS THE FINAL CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefs(coeffs): #this code works but it is not very elegant??\n",
    "    coeffs = np.mean((np.asarray(coeffs)), axis=0)\n",
    "\n",
    "    feature_names = data.iloc[:,4:].columns #get feature names from original data\n",
    "    coeffs = pd.DataFrame(data=coeffs, columns=feature_names)\n",
    "    coeffs = coeffs.transpose()\n",
    "\n",
    "    #get top positive and negative weights\n",
    "    largest_coefs = coeffs.nlargest(n=15, columns=0) \n",
    "    smallest_coefs = coeffs.nsmallest(n=15, columns=0)\n",
    "    \n",
    "    coeffs = pd.concat([largest_coefs,smallest_coefs], axis=0, sort=True) #concat for plotting\n",
    "    coeffs.plot(kind='barh', figsize=(8,10), legend=False) #plot weights\n",
    "    \n",
    "plot_coefs(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot above shows the most important features to classify a participant as a patient (positive weighted features) and as a control (negative weighted features). For example, a large third ventricle was more associated with patients. - CHECK IF THIS INTERPRETATION IS CORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Permutation testing\n",
    "\n",
    "EXPLAIN CODE HERE IF THIS IS THE FINAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(n_folds):\n",
    "    print(\"Loading models \", i)\n",
    "    f = open(\"./results/\"+experiment_name+\"/clf_\"+str(i)+\".pkl\", 'rb')\n",
    "    models.append(pickle.load(f))\n",
    "    f.close()\n",
    "\n",
    "f = open(\"./results/\"+experiment_name+\"/final_BAC.pkl\", 'rb')\n",
    "best_BAC = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLAIN NEXT CODE HERE IF THIS IS THE FINAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "seed=1\n",
    "n_permutation = 5\n",
    "permutation_test_bac = np.zeros((n_permutation,))\n",
    "\n",
    "print(\"Starting permutation with \", n_permutation, \" iterations\")\n",
    "for p in range(n_permutation):\n",
    "\n",
    "    np.random.seed(seed + p)\n",
    "    permuted_labels = np.random.permutation(y)\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    cv_test_bac = np.zeros((n_folds,))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = permuted_labels[train_index], permuted_labels[test_index]\n",
    "\n",
    "        # ------------------------------- Normalization ------------------------------------------------\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        clf = clone(models[i])\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predicted = clf.predict(X_test)\n",
    "\n",
    "        # -------------------------- Performance metrics -------------------------------------------\n",
    "        cm = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "        test_bac = np.sum(np.true_divide(np.diagonal(cm), np.sum(cm, axis=1))) / cm.shape[1]\n",
    "        cv_test_bac[i - 1] = test_bac\n",
    "\n",
    "    permutation_test_bac[p] = cv_test_bac.mean()\n",
    "    print(\"Permutation: \", p, \" BAC: \", cv_test_bac.mean())\n",
    "\n",
    "print(\"\")\n",
    "print(\"BEST BAC\", best_BAC.mean())\n",
    "pvalue = (np.sum((permutation_test_bac>best_BAC.mean()).astype('int'))+1.)/(n_permutation+1.)\n",
    "print(\"P-VALUE\", pvalue)\n",
    "\n",
    "#Save p-value to the designated folder\n",
    "f = open(\"./results/\"+experiment_name+\"/p-value.pkl\", 'wb')\n",
    "pickle.dump(pvalue, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
