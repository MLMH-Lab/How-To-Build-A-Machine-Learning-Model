{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19. Tutorial\n",
    "\n",
    "In this tutorial, we will implement a complete machine learning pipeline. The aim of this code is to show the facility and practicality to implement your pipelines in Python.\n",
    "\n",
    "Ok. Before we even start to implement our pipeline, it is necessary to perform some previous steps in our code.\n",
    "\n",
    "recorte\n",
    "This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.\n",
    "\n",
    "Following is the code to do this.\n",
    "Let’s start by getting your hands on the data. \n",
    "\n",
    "The Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made available by Kaggle as part of a computer-vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t already have one—don’t worry, the process is painless)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing helper libraries\n",
    "\n",
    "It is good practice to begin the code file by importing all the libraries you will need in your analysis. In this tutorial, we will use the following main libraries:\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - matplotlib and seaborn\n",
    "    - scikit-learn\n",
    "    \n",
    "To make the code clearer, it is common to import certain libraries and assigning it a nickname. For example, the  pandas library is typically imported as **pd**. This way, we simply type pd every time we want to call the pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store and organize output files\n",
    "from pathlib import Path\n",
    "\n",
    "#Manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Statistical tests\n",
    "import scipy.stats as stats\n",
    "\n",
    "#Machine learning\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "# Ignore WARNING\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed\n",
    "\n",
    "In our analysis, there will be some elements of randomness. For example, we might want to remove some participants at random during data cleaning. Likewise, when defining the cross-validation scheme (CV), the train/test partition at each iteration is also done at random. In Python, this randomness can be controlled by setting a \"seed value\" to a fixed value. Not defining a specific seed value means that the variables that rely on this element of randomness will have a different value every time you run your code. For example, the division of participants between the train and test sets will be different, which will likely lead to a different result.\n",
    "\n",
    "Setting the seed value guarantees that you will get the same results every time you run your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "np.random.seed = random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize your workspace\n",
    "\n",
    "First, we will create the structure of folders where you can store all your results. During your projects, you might test different strategies to perform the machine learning pipeline. As we mentioned before, the machine learning analysis is performed by several trials and errors (e.g., try different models, try different preprocessing, etc.). After a lot of testing, it will be easy to feel lost. So, give your experiment a name, create a folder in the results directory with the same name, and store your experiment outputs is this local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('./results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "experiment_name = 'linear_SVM_example'\n",
    "experiment_dir = results_dir / experiment_name\n",
    "experiment_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem formulation\n",
    "With your environment set, we can perform the first step in your machine learning pipeline: *the definition of our machine learning problem*.\n",
    "\n",
    "Usually, this step is performed even before we start to write the source code. The goals of our analysis should be discussed when defining our research question; when you are defining your project scope. This will help to orient what you will do and not do, which data we should collect, and what kind of analysis we will perform. In this tutorial, our machine learning problem is: \n",
    "\n",
    "*We will use structural MRI data to classify subjects in two groups: healthy control and patients with schizophrenia.*\n",
    "\n",
    "**Consider adding a Figure very high level showing the problem**\n",
    "\n",
    "In the previous sentence, we can find all the important elements when defining a machine learning problem. We have:\n",
    "\n",
    "- **Features**: Structural MRI data\n",
    "- **Task**: Classification\n",
    "- **Target**: Patients with schizophrenia and Healthy Control\n",
    "\n",
    "Having your problem well framed is essential when conducting any kind of project, and it makes easier to share the objectives of your experiments with other team members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "In this step, we will start getting your hands on the data. The idea is to perform a series of analysis to get our data ready for the machine learning models. For this reason, the first steps will involve reading the data and performing a lot of exploratory analysis. Different analyses should be involved depending on the nature of your problem and the type of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loading the data\n",
    "\n",
    "In this example, we will use tabular data with columns as the features, targets and demographic data, and rows as participants. The data are stored as a Comma-Separated Values (CSV) file. We will use the library pandas to load and explore the data. \n",
    "\n",
    "From pandas, we use the function read_csv() to load the csv file containing our data. This will load the data into an object type called dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decide how we load the data in the text version of the book: 1) provide a download link, 2) run code to download it automatically into a specific directory using !wget command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(\"./Chapter_19_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by seeing the first six rows of the data. Selecting subsections (\"slicing\") of a dataframe using pandas is straightforward. There are different ways to do this. Here, we simply retrieve the six rows from the dataframe data (note that the first column is indexed 0 and the last column is not included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Left Lateral Ventricle</th>\n",
       "      <th>Left Inf Lat Vent</th>\n",
       "      <th>Left Cerebellum White Matter</th>\n",
       "      <th>Left Cerebellum Cortex</th>\n",
       "      <th>Left Thalamus Proper</th>\n",
       "      <th>Left Caudate</th>\n",
       "      <th>...</th>\n",
       "      <th>rh rostralanteriorcingulate thickness</th>\n",
       "      <th>rh rostralmiddlefrontal thickness</th>\n",
       "      <th>rh superiorfrontal thickness</th>\n",
       "      <th>rh superiorparietal thickness</th>\n",
       "      <th>rh superiortemporal thickness</th>\n",
       "      <th>rh supramarginal thickness</th>\n",
       "      <th>rh frontalpole thickness</th>\n",
       "      <th>rh temporalpole thickness</th>\n",
       "      <th>rh transversetemporal thickness</th>\n",
       "      <th>rh insula thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.176965</td>\n",
       "      <td>4914.012999</td>\n",
       "      <td>277.612999</td>\n",
       "      <td>13639.31300</td>\n",
       "      <td>45034.11300</td>\n",
       "      <td>5921.012999</td>\n",
       "      <td>3816.512999</td>\n",
       "      <td>...</td>\n",
       "      <td>2.123999</td>\n",
       "      <td>2.367999</td>\n",
       "      <td>2.631999</td>\n",
       "      <td>2.065999</td>\n",
       "      <td>2.353999</td>\n",
       "      <td>2.429999</td>\n",
       "      <td>2.346999</td>\n",
       "      <td>2.205999</td>\n",
       "      <td>2.007999</td>\n",
       "      <td>2.508999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c060</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.423204</td>\n",
       "      <td>3702.102164</td>\n",
       "      <td>91.702164</td>\n",
       "      <td>12921.20216</td>\n",
       "      <td>41767.80216</td>\n",
       "      <td>6960.302164</td>\n",
       "      <td>3444.202164</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152164</td>\n",
       "      <td>2.277164</td>\n",
       "      <td>2.823164</td>\n",
       "      <td>2.143164</td>\n",
       "      <td>2.526164</td>\n",
       "      <td>2.487164</td>\n",
       "      <td>2.726164</td>\n",
       "      <td>2.926164</td>\n",
       "      <td>2.030164</td>\n",
       "      <td>2.550164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.749615</td>\n",
       "      <td>10267.711230</td>\n",
       "      <td>259.511229</td>\n",
       "      <td>15274.71123</td>\n",
       "      <td>56189.71123</td>\n",
       "      <td>8194.011229</td>\n",
       "      <td>3405.911229</td>\n",
       "      <td>...</td>\n",
       "      <td>2.971229</td>\n",
       "      <td>2.310229</td>\n",
       "      <td>2.736229</td>\n",
       "      <td>2.142229</td>\n",
       "      <td>3.063229</td>\n",
       "      <td>2.746229</td>\n",
       "      <td>2.953229</td>\n",
       "      <td>4.066229</td>\n",
       "      <td>2.419229</td>\n",
       "      <td>3.088229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694650</td>\n",
       "      <td>4492.206313</td>\n",
       "      <td>107.106313</td>\n",
       "      <td>16172.30631</td>\n",
       "      <td>46163.10631</td>\n",
       "      <td>7103.306313</td>\n",
       "      <td>4027.606313</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271313</td>\n",
       "      <td>2.561313</td>\n",
       "      <td>2.783313</td>\n",
       "      <td>2.213313</td>\n",
       "      <td>2.392313</td>\n",
       "      <td>2.413313</td>\n",
       "      <td>2.882313</td>\n",
       "      <td>2.388313</td>\n",
       "      <td>2.581313</td>\n",
       "      <td>2.745313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.964645</td>\n",
       "      <td>4270.705179</td>\n",
       "      <td>298.305179</td>\n",
       "      <td>12021.10518</td>\n",
       "      <td>45176.30518</td>\n",
       "      <td>7052.405179</td>\n",
       "      <td>3224.505179</td>\n",
       "      <td>...</td>\n",
       "      <td>2.715179</td>\n",
       "      <td>2.353179</td>\n",
       "      <td>2.789179</td>\n",
       "      <td>2.156179</td>\n",
       "      <td>2.879179</td>\n",
       "      <td>2.489179</td>\n",
       "      <td>2.765179</td>\n",
       "      <td>3.502179</td>\n",
       "      <td>2.292179</td>\n",
       "      <td>2.834179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c365</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.684447</td>\n",
       "      <td>4107.105258</td>\n",
       "      <td>121.405258</td>\n",
       "      <td>14418.80526</td>\n",
       "      <td>65958.90526</td>\n",
       "      <td>10132.905260</td>\n",
       "      <td>3437.205258</td>\n",
       "      <td>...</td>\n",
       "      <td>3.079258</td>\n",
       "      <td>2.656258</td>\n",
       "      <td>2.930258</td>\n",
       "      <td>2.263258</td>\n",
       "      <td>3.067258</td>\n",
       "      <td>2.721258</td>\n",
       "      <td>2.527258</td>\n",
       "      <td>4.275258</td>\n",
       "      <td>2.520258</td>\n",
       "      <td>3.106258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  label  Gender        Age  Left Lateral Ventricle  Left Inf Lat Vent  \\\n",
       "0  c045      0       0  23.176965             4914.012999         277.612999   \n",
       "1  c060      0       1  25.423204             3702.102164          91.702164   \n",
       "2  c248      0       0  24.749615            10267.711230         259.511229   \n",
       "3  c009      0       0  21.694650             4492.206313         107.106313   \n",
       "4  c030      0       0  23.964645             4270.705179         298.305179   \n",
       "5  c365      0       0  25.684447             4107.105258         121.405258   \n",
       "\n",
       "   Left Cerebellum White Matter  Left Cerebellum Cortex  Left Thalamus Proper  \\\n",
       "0                   13639.31300             45034.11300           5921.012999   \n",
       "1                   12921.20216             41767.80216           6960.302164   \n",
       "2                   15274.71123             56189.71123           8194.011229   \n",
       "3                   16172.30631             46163.10631           7103.306313   \n",
       "4                   12021.10518             45176.30518           7052.405179   \n",
       "5                   14418.80526             65958.90526          10132.905260   \n",
       "\n",
       "   Left Caudate  ...  rh rostralanteriorcingulate thickness  \\\n",
       "0   3816.512999  ...                               2.123999   \n",
       "1   3444.202164  ...                               2.152164   \n",
       "2   3405.911229  ...                               2.971229   \n",
       "3   4027.606313  ...                               2.271313   \n",
       "4   3224.505179  ...                               2.715179   \n",
       "5   3437.205258  ...                               3.079258   \n",
       "\n",
       "   rh rostralmiddlefrontal thickness  rh superiorfrontal thickness  \\\n",
       "0                           2.367999                      2.631999   \n",
       "1                           2.277164                      2.823164   \n",
       "2                           2.310229                      2.736229   \n",
       "3                           2.561313                      2.783313   \n",
       "4                           2.353179                      2.789179   \n",
       "5                           2.656258                      2.930258   \n",
       "\n",
       "   rh superiorparietal thickness  rh superiortemporal thickness  \\\n",
       "0                       2.065999                       2.353999   \n",
       "1                       2.143164                       2.526164   \n",
       "2                       2.142229                       3.063229   \n",
       "3                       2.213313                       2.392313   \n",
       "4                       2.156179                       2.879179   \n",
       "5                       2.263258                       3.067258   \n",
       "\n",
       "   rh supramarginal thickness  rh frontalpole thickness  \\\n",
       "0                    2.429999                  2.346999   \n",
       "1                    2.487164                  2.726164   \n",
       "2                    2.746229                  2.953229   \n",
       "3                    2.413313                  2.882313   \n",
       "4                    2.489179                  2.765179   \n",
       "5                    2.721258                  2.527258   \n",
       "\n",
       "   rh temporalpole thickness  rh transversetemporal thickness  \\\n",
       "0                   2.205999                         2.007999   \n",
       "1                   2.926164                         2.030164   \n",
       "2                   4.066229                         2.419229   \n",
       "3                   2.388313                         2.581313   \n",
       "4                   3.502179                         2.292179   \n",
       "5                   4.275258                         2.520258   \n",
       "\n",
       "   rh insula thickness  \n",
       "0             2.508999  \n",
       "1             2.550164  \n",
       "2             3.088229  \n",
       "3             2.745313  \n",
       "4             2.834179  \n",
       "5             3.106258  \n",
       "\n",
       "[6 rows x 173 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> dataset_df[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the dataset_df variable, we can see the column names at the top and the contents of the first six participants. The columns include the ID, age and gender, as well as the gray matter volume for several brain regions. We can see that there is at least one value missing. We will deal with this later. \n",
    "\n",
    "It may be useful to know all the features that are available in the dataset. To do this, we simply ask for the names of the columns of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'label',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Left Lateral Ventricle',\n",
       " 'Left Inf Lat Vent',\n",
       " 'Left Cerebellum White Matter',\n",
       " 'Left Cerebellum Cortex',\n",
       " 'Left Thalamus Proper',\n",
       " 'Left Caudate',\n",
       " 'Left Putamen',\n",
       " 'Left Pallidum',\n",
       " 'rd Ventricle',\n",
       " 'th Ventricle',\n",
       " 'Brain Stem',\n",
       " 'Left Hippocampus',\n",
       " 'Left Amygdala',\n",
       " 'CSF',\n",
       " 'Left Accumbens area',\n",
       " 'Left VentralDC',\n",
       " 'Right Lateral Ventricle',\n",
       " 'Right Inf Lat Vent',\n",
       " 'Right Cerebellum White Matter',\n",
       " 'Right Cerebellum Cortex',\n",
       " 'Right Thalamus Proper',\n",
       " 'Right Caudate',\n",
       " 'Right Putamen',\n",
       " 'Right Pallidum',\n",
       " 'Right Hippocampus',\n",
       " 'Right Amygdala',\n",
       " 'Right Accumbens area',\n",
       " 'Right VentralDC',\n",
       " 'CC Posterior',\n",
       " 'CC Mid Posterior',\n",
       " 'CC Central',\n",
       " 'CC Mid Anterior',\n",
       " 'CC Anterior',\n",
       " 'lh bankssts volume',\n",
       " 'lh caudalanteriorcingulate volume',\n",
       " 'lh caudalmiddlefrontal volume',\n",
       " 'lh cuneus volume',\n",
       " 'lh entorhinal volume',\n",
       " 'lh fusiform volume',\n",
       " 'lh inferiorparietal volume',\n",
       " 'lh inferiortemporal volume',\n",
       " 'lh isthmuscingulate volume',\n",
       " 'lh lateraloccipital volume',\n",
       " 'lh lateralorbitofrontal volume',\n",
       " 'lh lingual volume',\n",
       " 'lh medialorbitofrontal volume',\n",
       " 'lh middletemporal volume',\n",
       " 'lh parahippocampal volume',\n",
       " 'lh paracentral volume',\n",
       " 'lh parsopercularis volume',\n",
       " 'lh parsorbitalis volume',\n",
       " 'lh parstriangularis volume',\n",
       " 'lh pericalcarine volume',\n",
       " 'lh postcentral volume',\n",
       " 'lh posteriorcingulate volume',\n",
       " 'lh precentral volume',\n",
       " 'lh precuneus volume',\n",
       " 'lh rostralanteriorcingulate volume',\n",
       " 'lh rostralmiddlefrontal volume',\n",
       " 'lh superiorfrontal volume',\n",
       " 'lh superiorparietal volume',\n",
       " 'lh superiortemporal volume',\n",
       " 'lh supramarginal volume',\n",
       " 'lh frontalpole volume',\n",
       " 'lh temporalpole volume',\n",
       " 'lh transversetemporal volume',\n",
       " 'lh insula volume',\n",
       " 'rh bankssts volume',\n",
       " 'rh caudalanteriorcingulate volume',\n",
       " 'rh caudalmiddlefrontal volume',\n",
       " 'rh cuneus volume',\n",
       " 'rh entorhinal volume',\n",
       " 'rh fusiform volume',\n",
       " 'rh inferiorparietal volume',\n",
       " 'rh inferiortemporal volume',\n",
       " 'rh isthmuscingulate volume',\n",
       " 'rh lateraloccipital volume',\n",
       " 'rh lateralorbitofrontal volume',\n",
       " 'rh lingual volume',\n",
       " 'rh medialorbitofrontal volume',\n",
       " 'rh middletemporal volume',\n",
       " 'rh parahippocampal volume',\n",
       " 'rh paracentral volume',\n",
       " 'rh parsopercularis volume',\n",
       " 'rh parsorbitalis volume',\n",
       " 'rh parstriangularis volume',\n",
       " 'rh pericalcarine volume',\n",
       " 'rh postcentral volume',\n",
       " 'rh posteriorcingulate volume',\n",
       " 'rh precentral volume',\n",
       " 'rh precuneus volume',\n",
       " 'rh rostralanteriorcingulate volume',\n",
       " 'rh rostralmiddlefrontal volume',\n",
       " 'rh superiorfrontal volume',\n",
       " 'rh superiorparietal volume',\n",
       " 'rh superiortemporal volume',\n",
       " 'rh supramarginal volume',\n",
       " 'rh frontalpole volume',\n",
       " 'rh temporalpole volume',\n",
       " 'rh transversetemporal volume',\n",
       " 'rh insula volume',\n",
       " 'lh bankssts thickness',\n",
       " 'lh caudalanteriorcingulate thickness',\n",
       " 'lh caudalmiddlefrontal thickness',\n",
       " 'lh cuneus thickness',\n",
       " 'lh entorhinal thickness',\n",
       " 'lh fusiform thickness',\n",
       " 'lh inferiorparietal thickness',\n",
       " 'lh inferiortemporal thickness',\n",
       " 'lh isthmuscingulate thickness',\n",
       " 'lh lateraloccipital thickness',\n",
       " 'lh lateralorbitofrontal thickness',\n",
       " 'lh lingual thickness',\n",
       " 'lh medialorbitofrontal thickness',\n",
       " 'lh middletemporal thickness',\n",
       " 'lh parahippocampal thickness',\n",
       " 'lh paracentral thickness',\n",
       " 'lh parsopercularis thickness',\n",
       " 'lh parsorbitalis thickness',\n",
       " 'lh parstriangularis thickness',\n",
       " 'lh pericalcarine thickness',\n",
       " 'lh postcentral thickness',\n",
       " 'lh posteriorcingulate thickness',\n",
       " 'lh precentral thickness',\n",
       " 'lh precuneus thickness',\n",
       " 'lh rostralanteriorcingulate thickness',\n",
       " 'lh rostralmiddlefrontal thickness',\n",
       " 'lh superiorfrontal thickness',\n",
       " 'lh superiorparietal thickness',\n",
       " 'lh superiortemporal thickness',\n",
       " 'lh supramarginal thickness',\n",
       " 'lh frontalpole thickness',\n",
       " 'lh temporalpole thickness',\n",
       " 'lh transversetemporal thickness',\n",
       " 'lh insula thickness',\n",
       " 'rh bankssts thickness',\n",
       " 'rh caudalanteriorcingulate thickness',\n",
       " 'rh caudalmiddlefrontal thickness',\n",
       " 'rh cuneus thickness',\n",
       " 'rh entorhinal thickness',\n",
       " 'rh fusiform thickness',\n",
       " 'rh inferiorparietal thickness',\n",
       " 'rh inferiortemporal thickness',\n",
       " 'rh isthmuscingulate thickness',\n",
       " 'rh lateraloccipital thickness',\n",
       " 'rh lateralorbitofrontal thickness',\n",
       " 'rh lingual thickness',\n",
       " 'rh medialorbitofrontal thickness',\n",
       " 'rh middletemporal thickness',\n",
       " 'rh parahippocampal thickness',\n",
       " 'rh paracentral thickness',\n",
       " 'rh parsopercularis thickness',\n",
       " 'rh parsorbitalis thickness',\n",
       " 'rh parstriangularis thickness',\n",
       " 'rh pericalcarine thickness',\n",
       " 'rh postcentral thickness',\n",
       " 'rh posteriorcingulate thickness',\n",
       " 'rh precentral thickness',\n",
       " 'rh precuneus thickness',\n",
       " 'rh rostralanteriorcingulate thickness',\n",
       " 'rh rostralmiddlefrontal thickness',\n",
       " 'rh superiorfrontal thickness',\n",
       " 'rh superiorparietal thickness',\n",
       " 'rh superiortemporal thickness',\n",
       " 'rh supramarginal thickness',\n",
       " 'rh frontalpole thickness',\n",
       " 'rh temporalpole thickness',\n",
       " 'rh transversetemporal thickness',\n",
       " 'rh insula thickness']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> dataset_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what features we are dealing with, let's check the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features = 173\n",
      "Number of participants = 740\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features =\", dataset_df.shape[1])\n",
    "print(\"Number of participants =\", dataset_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, our data preparation stage will check the dataset for the presence of:\n",
    " - Missing data \n",
    " - Data imbalance with respect to the labels\n",
    " - Confounding variables\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Missing data\n",
    "\n",
    "Many machine learning algorithms do not support data with missing values. Therefore, it is important to check if there are any missing values in our data. There are many different ways to do this. Here we will build our own function to loop through each the column in the dataframe data (note data we are also including gender and age) and get the feature name and Id for the corresponding missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAFAEL DO REVIEW THIS FUNCTION\n",
    "\n",
    "def detect_nan(dataset):\n",
    "    nan_total = dataset.isnull().sum().sum() \n",
    "    if nan_total > 0:\n",
    "        for column in dataset:          \n",
    "            #Find Ids with nan - THIS IS PROBABLY OVERLY COMPLICATED (all I want here is to get the Ids of where the nans are so I can print them later on)\n",
    "            nan = dataset[column].isnull()\n",
    "            dataset[\"nan\"] = nan \n",
    "            ids = []\n",
    "            for i in dataset[\"nan\"]:\n",
    "                if i == True:\n",
    "                    id_nan = dataset.loc[dataset[\"nan\"] == True, 'ID']\n",
    "                    ids.append(id_nan)               \n",
    "            #Calculate total number of nan for each feature and Id\n",
    "            nan_sum = nan.sum()          \n",
    "            if nan_sum > 0:\n",
    "                print(\"Found\", nan_sum, \"missing value(s) for\", column, \"for Id(s):\", *ids[0])             \n",
    "        #dataset = dataset.drop(columns=[\"nan\"])\n",
    "    else:\n",
    "        print(\"There are no missing data in this dataset!\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 missing value(s) for Age for Id(s): p149 p150 p156 p157 p168 p175 p195 p196 p197 p210 p211 p212 p227 p228 p229 p264 p265 p266 p267 p268 p269 p270 p271 p281 p282 p283 p289 p302 p303 p307 p311 p312 p319 p321 p356 p357 p358 p359 p360 p361 p362 p363 p364\n"
     ]
    }
   ],
   "source": [
    ">>> detect_nan(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can check that there are 12 missing values for gender and age. There are several options to go from here with different degrees of complexity (see Chapter x for a more in-depth description). In this example, we will simply remove the participants with missing data. Not having this information does not allow for a thorough assessment of imbalanced demographic data, which could be problematic. There are sophisticated options to input missing data however they all come with disadvantages. Since removing these participants would only result in losing x% of the total data, this option will not result in a large drop in sample size. We can remove missing data using the function .dropna() from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants = 697\n"
     ]
    }
   ],
   "source": [
    "dataset_df = dataset_df.dropna()\n",
    "\n",
    "print(\"Number of participants =\", dataset_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the new dataframe has now x less participants than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Class imbalance\n",
    "\n",
    "Next, let's check the number of total participants in each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    368\n",
       "1    329\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> dataset_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that there are 740 participants in total, 368 patients and 372 controls. There does not seem to be a large imbalance between classes. However, they are not perfectly matched. One option would be to downsample the HC to match the SZ group. However, this would mean losing some data. Since the imbalance is not too large, we will use balanced accuracy as our metric of choice as well as a stratified CV to ensure the same proportion SZ/HC across the CV iterations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Confounding variables\n",
    "\n",
    "Next, let's check for the balance of some obvious confounding variables: gender and age. First, let's see the distribution of gender between the two classes. This time, we will use some plots from the seaborn library. Plotting data using seaborn is straightforward. Seaborn provides a high-level interface for drawing attractive and informative statistical graphics. For more information see https://seaborn.pydata.org. Seaborn operates  based on matplotlib, the most widely used plotting library. Therefore, we will also import it. You can find out more about matplotlib here https://matplotlib.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFs9JREFUeJzt3X+0VeV95/H3V0QxaqPRW0tBveiiJqYwoBeMNjH4Y0RJR2p+GIiNWMOQTDWJSTVj7HJ0XDptU20m4kSDS4smiqZSlSROKzqiSVYpXAJBlIiaYLwW8QazMBoxAt/5427wcNnIAe45++J5v9Y66+797P2c8z2sy/2s/eznnCcyE0mSetuj6gIkSf2TASFJKmVASJJKGRCSpFIGhCSplAEhSSplQEiSShkQkqRSBoQkqdSeVRewKw4++OBsb2+vugxJ2q0sWrToV5nZtr3zduuAaG9vp7Ozs+oyJGm3EhHP1XOeQ0ySpFIGhCSplAEhSSq1W9+DkKQ333yTrq4u1q1bV3Up/c6gQYMYOnQoAwcO3Kn+BoSk3VpXVxf7778/7e3tRETV5fQbmcmaNWvo6upi2LBhO/UcDjFJ2q2tW7eOgw46yHDoJSI46KCDdunKyoCQtNszHMrt6r+LASFJKuU9CEnvKH/zv7/Zp8/31Yv+crvnRATnnHMO3/nOdwBYv349gwcP5rjjjuP73//+NvvNmzePa6+99m3PqZIBIfVTff2HbndWzx/pKu27774sW7aM119/nX322Ye5c+cyZMiQqsvaZQ4xSVIfmDBhAj/4wQ8AmDVrFpMnT958bMGCBRx//PGMHj2aE044gaeeemqr/q+99hrnn38+Y8eOZfTo0dx///1Nq31bDAhJ6gOTJk3irrvuYt26dSxdupTjjjtu87H3vve9/PCHP2Tx4sVcddVVXHbZZVv1v+aaazj55JNZsGABjzzyCJdccgmvvfZaM9/CVlp+iMnL+Lf098t4qT8bOXIkK1euZNasWUyYMGGLY2vXrmXKlCk8/fTTRARvvvnmVv0ffPBB5syZw7XXXgv0TN/95S9/yfve976m1F+mYQEREYcCtwOHAAnMyMxvRMR7gLuBdmAlcHZm/jp65mN9A5gA/BY4LzN/0qj6JKmvnXnmmVx88cXMmzePNWvWbG6//PLLOemkk7j33ntZuXIl48aN26pvZjJ79myOOuqoJlb89ho5xLQe+KvMPBr4AHBBRBwNXAo8nJnDgYeLfYAzgOHFYxpwYwNrk6Q+d/7553PFFVcwYsSILdrXrl27+ab1zJkzS/uOHz+e6dOnk5kALF68uKG11qNhVxCZuQpYVWz/JiKWA0OAicC44rTbgHnAfy/ab8+ef535EXFARAwunkeS6lLlUOnQoUP5whe+sFX7V77yFaZMmcLVV1/NRz7ykdK+l19+ORdddBEjR45k48aNDBs2rPLpr025BxER7cBo4N+BQ2r+6L9IzxAU9ITH8zXduoo2A0JSv/bqq69u1TZu3LjNQ0nHH388K1as2Hzs6quv3uqcffbZh29961sNr3VHNHwWU0TsB8wGLsrMV2qPFVcLuYPPNy0iOiOis7u7uw8rlSTVamhARMRAesLhjsz856J5dUQMLo4PBl4q2l8ADq3pPrRo20JmzsjMjszsaGvb7pKqkqSd1LCAKGYl3QIsz8x/qDk0B5hSbE8B7q9pPzd6fABY6/0HSapOI+9B/AnwaeDxiFhStF0G/C3w3Yj4DPAccHZx7AF6prg+Q880179oYG2SpO1o5CymHwHb+q7ZU0rOT+CCRtUjSdoxftWGJKlUy3/VhqR3lucWfKpPn+/wsXdu95wBAwZs8eG4++67j/b29j6tY5OZM2fS2dnJDTfc0JDnr2VASNIu2meffViyZMn2T9zNOMQkSQ2wYcMGLrnkEsaMGcPIkSM3fwhu3rx5fPjDH2bixIkcccQRXHrppdxxxx2MHTuWESNG8OyzzwLwve99j+OOO47Ro0dz6qmnsnr16q1eo7u7m4997GOMGTOGMWPG8OMf/7hP34MBIUm76PXXX2fUqFGMGjWKs846C4BbbrmFd7/73SxcuJCFCxdy880384tf/AKAn/70p9x0000sX76cb3/726xYsYIFCxYwdepUpk+fDsAHP/hB5s+fz+LFi5k0aRJf+9rXtnrdL37xi3zpS19i4cKFzJ49m6lTp/bp+3KISZJ2UdkQ04MPPsjSpUu55557gJ4v7Hv66afZa6+9GDNmDIMHDwbgyCOP5LTTTgNgxIgRPPLIIwB0dXXxyU9+klWrVvG73/2OYcOGbfW6Dz30EE8++eTm/VdeeYVXX32V/fbbr0/elwEhSQ2QmUyfPp3x48dv0T5v3jz23nvvzft77LHH5v099tiD9evXA/D5z3+eL3/5y5x55pnMmzePK6+8cqvX2LhxI/Pnz2fQoEENeQ8OMUlSA4wfP54bb7xx8+JAK1as2KEV4mq/Ivy2224rPee0007bPCQF9PmNcq8gJL2j1DMttRmmTp3KypUrOeaYY8hM2trauO++++ruf+WVV/KJT3yCAw88kJNPPnnz/Yta119/PRdccAEjR45k/fr1nHjiidx000199h5i0+IUu6OOjo7s7OzcpedwydG3uORo/+Lv5lve7ndz+fLllS7L2d+V/ftExKLM7NheX4eYJEmlDAhJUikDQtJub3ceKm+kXf13MSAk7dYGDRrEmjVrDIleMpM1a9bs0hRYZzFJ2q0NHTqUrq4uXIJ4a4MGDWLo0KE73d+AkLRbGzhwYOmnjLXrGrnk6K0R8VJELKtpuzsilhSPlZtWmouI9oh4veZY303klSTtlEZeQcwEbgBu39SQmZ/ctB0R1wFra85/NjNHNbAeSdIOaOSSo49FRHvZsYgIetaiPrlRry9J2jVVzWL6ELA6M5+uaRsWEYsj4tGI+FBFdUmSClXdpJ4MzKrZXwUclplrIuJY4L6IeH9mvtK7Y0RMA6YBHHbYYU0pVpJaUdOvICJiT+CjwN2b2jLzjcxcU2wvAp4F/qisf2bOyMyOzOxoa2trRsmS1JKqGGI6FfhZZnZtaoiItogYUGwfAQwHfl5BbZKkQiOnuc4C/g04KiK6IuIzxaFJbDm8BHAisLSY9noP8LnMfLlRtUmStq+Rs5gmb6P9vJK22cDsRtUiSdpxfheTJKmUASFJKmVASJJKGRCSpFIGhCSplAEhSSplQEiSSrlgkDZ7bsGnqi6h3zh87J1VlyBVzisISVIpA0KSVMqAkCSVMiAkSaUMCElSKQNCklTKgJAklTIgJEmlGrmi3K0R8VJELKtpuzIiXoiIJcVjQs2xr0bEMxHxVESMb1RdkqT6NPIKYiZwekn71zNzVPF4ACAijqZnKdL3F32+uWmNaklSNRoWEJn5GFDvutITgbsy843M/AXwDDC2UbVJkravinsQF0bE0mII6sCibQjwfM05XUXbViJiWkR0RkRnd3d3o2uVpJbV7IC4ETgSGAWsAq7b0SfIzBmZ2ZGZHW1tbX1dnySp0NSAyMzVmbkhMzcCN/PWMNILwKE1pw4t2iRJFWlqQETE4Jrds4BNM5zmAJMiYu+IGAYMBxY0szZJ0pYath5ERMwCxgEHR0QXcAUwLiJGAQmsBD4LkJlPRMR3gSeB9cAFmbmhUbVJkravYQGRmZNLmm95m/OvAa5pVD2SpB3jJ6klSaUMCElSKQNCklTKgJAklTIgJEmlDAhJUikDQpJUyoCQJJUyICRJpQwISVIpA0KSVMqAkCSVMiAkSaUMCElSKQNCklTKgJAklWpYQETErRHxUkQsq2n7+4j4WUQsjYh7I+KAor09Il6PiCXF46ZG1SVJqk8jryBmAqf3apsL/HFmjgRWAF+tOfZsZo4qHp9rYF2SpDo0LCAy8zHg5V5tD2bm+mJ3PjC0Ua8vSdo1Vd6DOB/4vzX7wyJicUQ8GhEf2laniJgWEZ0R0dnd3d34KiWpRVUSEBHx18B64I6iaRVwWGaOBr4M3BkRv1fWNzNnZGZHZna0tbU1p2BJakFND4iIOA/4U+CczEyAzHwjM9cU24uAZ4E/anZtkqS3NDUgIuJ04CvAmZn525r2togYUGwfAQwHft7M2iRJW9qznpMi4uHMPGV7bb2OzwLGAQdHRBdwBT2zlvYG5kYEwPxixtKJwFUR8SawEfhcZr5c+sSSWs5zCz5VdQn9xuFj72zaa71tQETEIOBd9PyRPxCI4tDvAUPerm9mTi5pvmUb584GZm+3WklS02zvCuKzwEXAHwKLeCsgXgFuaGBdkqSKvW1AZOY3gG9ExOczc3qTapIk9QN13YPIzOkRcQLQXtsnM29vUF2SpIrVe5P628CRwBJgQ9GcgAEhSe9QdQUE0AEcvelzC5Kkd756PwexDPiDRhYiSepf6r2COBh4MiIWAG9saszMMxtSlSSpcvUGxJWNLEKS1P/UO4vp0UYXIknqX+qdxfQbemYtAewFDARey8zSb1yVJO3+6r2C2H/TdvR8idJE4AONKkqSVL0d/jbX7HEfML4B9UiS+ol6h5g+WrO7Bz2fi1jXkIokSf1CvbOY/kvN9npgJT3DTJKkd6h670H8RaMLkST1L3Xdg4iIoRFxb0S8VDxmR8TQRhcnSapOvTep/xGYQ8+6EH8IfK9oe1sRcWsRKMtq2t4TEXMj4uni54FFe0TE9RHxTEQsjYhjdvztSJL6Sr0B0ZaZ/5iZ64vHTKCtjn4zgdN7tV0KPJyZw4GHi32AM+hZi3o4MA24sc7aJEkNUG9ArImIP4+IAcXjz4E12+uUmY8BvdeWngjcVmzfBvxZTfvtxTTa+cABETG4zvokSX2s3oA4HzgbeBFYBXwcOG8nX/OQzFxVbL8IHFJsDwGerzmvi5J1ryNiWkR0RkRnd3f3TpYgSdqeegPiKmBKZrZl5u/TExj/c1dfvFhfYofWmMjMGZnZkZkdbW31jHJJknZGvQExMjN/vWknM18GRu/ka67eNHRU/HypaH8BOLTmvKFFmySpAvUGxB6bZhtBz0wk6v+QXW9zgCnF9hTg/pr2c4vZTB8A1tYMRUmSmqzeP/LXAf8WEf9U7H8CuGZ7nSJiFjAOODgiuoArgL8FvhsRnwGeo+feBsADwATgGeC3gB/Ok6QK1ftJ6tsjohM4uWj6aGY+WUe/yds4dErJuQlcUE89kqTGq3uYqAiE7YaCJOmdYYe/7luS1BoMCElSKQNCklTKgJAklTIgJEmlDAhJUikDQpJUyoCQJJUyICRJpQwISVIpA0KSVMqAkCSVMiAkSaUMCElSqZ1dFW6nRcRRwN01TUcA/wM4APivQHfRfllmPtDk8iRJhaYHRGY+BYwCiIgB9Kw7fS89K8h9PTOvbXZNkqStVT3EdArwbGY+V3EdkqReqg6IScCsmv0LI2JpRNwaEQdWVZQkqcKAiIi9gDOBfyqabgSOpGf4aRVw3Tb6TYuIzojo7O7uLjtFktQHqryCOAP4SWauBsjM1Zm5ITM3AjcDY8s6ZeaMzOzIzI62trYmlitJraXKgJhMzfBSRAyuOXYWsKzpFUmSNmv6LCaAiNgX+M/AZ2uavxYRo4AEVvY6JklqskoCIjNfAw7q1fbpKmqRJJWrehaTJKmfMiAkSaUMCElSKQNCklTKgJAklTIgJEmlDAhJUikDQpJUyoCQJJUyICRJpQwISVIpA0KSVMqAkCSVMiAkSaUMCElSKQNCklSqkgWDACJiJfAbYAOwPjM7IuI9wN1AOz2ryp2dmb+uqkZJamVVX0GclJmjMrOj2L8UeDgzhwMPF/uSpApUHRC9TQRuK7ZvA/6swlokqaVVGRAJPBgRiyJiWtF2SGauKrZfBA6ppjRJUmX3IIAPZuYLEfH7wNyI+FntwczMiMjenYowmQZw2GGHNadSSWpBlV1BZOYLxc+XgHuBscDqiBgMUPx8qaTfjMzsyMyOtra2ZpYsSS2lkoCIiH0jYv9N28BpwDJgDjClOG0KcH8V9UmSqhtiOgS4NyI21XBnZv5LRCwEvhsRnwGeA86uqD5JanmVBERm/hz4TyXta4BTml+RJKm3/jbNVZLUTxgQkqRSBoQkqZQBIUkqZUBIkkoZEJKkUgaEJKmUASFJKmVASJJKGRCSpFIGhCSplAEhSSplQEiSShkQkqRSBoQkqZQBIUkq1fSAiIhDI+KRiHgyIp6IiC8W7VdGxAsRsaR4TGh2bZKkt1Sxotx64K8y8yfFutSLImJucezrmXltBTVJknppekBk5ipgVbH9m4hYDgxpdh2SpLdX6T2IiGgHRgP/XjRdGBFLI+LWiDiwssIkSdUFRETsB8wGLsrMV4AbgSOBUfRcYVy3jX7TIqIzIjq7u7ubVq8ktZpKAiIiBtITDndk5j8DZObqzNyQmRuBm4GxZX0zc0ZmdmRmR1tbW/OKlqQWU8UspgBuAZZn5j/UtA+uOe0sYFmza5MkvaWKWUx/AnwaeDwilhRtlwGTI2IUkMBK4LMV1CZJKlQxi+lHQJQceqDZtUiSts1PUkuSShkQkqRSBoQkqZQBIUkqZUBIkkoZEJKkUgaEJKmUASFJKmVASJJKGRCSpFIGhCSplAEhSSplQEiSShkQkqRSBoQkqZQBIUkq1e8CIiJOj4inIuKZiLi06nokqVX1q4CIiAHA/wHOAI6mZxnSo6utSpJaU78KCGAs8Exm/jwzfwfcBUysuCZJakn9LSCGAM/X7HcVbZKkJtuz6gJ2VERMA6YVu69GxFNV1vNOchkcDPyq6jr6h1lVF6Aa/m7W6pPfzcPrOam/BcQLwKE1+0OLts0ycwYwo5lFtYqI6MzMjqrrkHrzd7Ma/W2IaSEwPCKGRcRewCRgTsU1SVJL6ldXEJm5PiIuBP4VGADcmplPVFyWJLWkfhUQAJn5APBA1XW0KIfu1F/5u1mByMyqa5Ak9UP97R6EJKmfMCBaUES82mv/vIi4oWb/3IhYFhGPR8TiiLi4+VWqVUXEX0fEExGxNCKWRMSHi5+1j1ci4u+qrvWdrt/dg1C1IuIM4CLgtMz8j4jYGzi34rLUIiLieOBPgWMy842IOBjYKzNH1Zwzgp77lF+vqMyWYUCot68CF2fmfwBk5hvAzdWWpBYyGPhV8XtHZm7x4biIGATcCVyQmS9WUF9L8SZ1C4qIDcDjNU3vAeZk5oUR8TIwLDPXVlOdWllE7Af8CHgX8BBwd2Y+WnP8emBgZv63ikpsKV5BtKbXe12ynwf4KVVVLjNfjYhjgQ8BJwF3R8SlmTmzGP48FTi20iJbiAGh3p6g5z/g/6u6ELWmzNwAzAPmRcTjwJSIeAD4FjAxM1+vsr5W4iwm9fY3wN9HxB8ARMReETG14prUIiLiqIgYXtM0CngOuBWYnpmLq6msNXkFoS1k5gMRcQjwUEQEkPT855SaYT9gekQcAKwHnqFnttJjwKERcU7NuXMz85IKamwZ3qSWJJVyiEmSVMqAkCSVMiAkSaUMCElSKQNCklTKgJB2QO9vwi053h4Ry3bwOWdGxMd3rTKp7xkQkqRSBoS0EyJiv4h4OCJ+UqybMbHm8J4RcUdELI+IeyLiXUWfYyPi0YhYFBH/GhGDKypfqosBIe2cdcBZmXkMPV8qd13xyXOAo4BvZub7gFeAv4yIgcB04OOZeSw9n06/poK6pbr5VRvSzgngf0XEicBGYAhwSHHs+cz8cbH9HeALwL8AfwzMLXJkALCqqRVLO8iAkHbOOUAbcGxmvhkRK4FBxbHe31+T9ATKE5l5fPNKlHaNQ0zSznk38FIRDicBh9ccO6xYOhPgU/QsgPMU0LapPSIGRsT7m1qxtIMMCGnn3AF0FOsVnAv8rObYU8AFEbEcOBC4MTN/B3wc+LuI+CmwBDihyTVLO8Rvc5UklfIKQpJUyoCQJJUyICRJpQwISVIpA0KSVMqAkCSVMiAkSaUMCElSqf8PAtplflPXDl0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(x=\"label\", hue=\"Gender\", data=dataset_df, palette=['#839098', '#f7d842'])\n",
    "\n",
    "plt.legend([\"Male\", \"Female\"])\n",
    "plt.xticks([0,1], [\"HC\", \"SZ\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a fairly similar number of males in the two groups. However, the control group has more females than the patient group. Let's run a Chi-squared test of homogeneity to check if this difference is statistically significant. In this case, we want to test the hipotheses that the proportion of women in the healthy control group is the same proportion of women in the patient group (or the proportion of men in the healthy control group is the same proportion of men in the patient group).\n",
    "\n",
    "$$H_{0}: Proportion\\ men/women_{Healthy control} =  Proportion\\ men/women_{Patients}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label     0    1\n",
      "Gender          \n",
      "0       205  208\n",
      "1       163  121\n",
      "\n",
      "chi2 = 4.064\n",
      "p-value = 0.044\n"
     ]
    }
   ],
   "source": [
    "# Create the contigency table\n",
    "tab = pd.crosstab(dataset_df[\"Gender\"], dataset_df[\"label\"])\n",
    "print(tab)\n",
    "print(\"\")\n",
    "\n",
    "# Perform the homogeneity test\n",
    "chi2, p_value, _, _ = stats.chi2_contingency(tab, correction=False)\n",
    "print('chi2 = %.3f' % chi2)  \n",
    "print('p-value = %.3f' % p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that there is indeed a statistically significant difference between the two classes with respect to gender (p-value < 0.05). This may be an issue, as the impact of gender on brain morphology has been well-established. Therefore, the machine learning model may use brain features that are associated with gender differences to distinguish between HC and SZ, as opposed to differences related to the disorder.\n",
    "\n",
    "To mitigate the bias that this imbalance would introduce in our model, we will downsample the number of females in the HC class by randomly selecting and removing the necessary amount of participants to match the number of females in the SZ group. From the output above we can see that there are 163 and 120 females in the HC and SZ groups, respectively. Therefore, we will remove 42 females from the HC group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053240432090730934\n",
      "label     0    1\n",
      "Gender          \n",
      "0       205  208\n",
      "1       161  121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while p_value < 0.05:\n",
    "    #select one female controls at random and get their indexes\n",
    "    scz_women = dataset_df[(dataset_df['label']==0) & (dataset_df['Gender'] == 1)]\n",
    "    indexes_to_remove = scz_women.sample(n=1, random_state=1).index\n",
    "    print('Droping %s'%str(indexes_to_remove))\n",
    "    #remove them from the data\n",
    "    dataset_df = dataset_df.drop(indexes_to_remove)\n",
    "    tab = pd.crosstab(dataset_df[\"Gender\"], dataset_df[\"label\"])\n",
    "    chi2, p_value, _, _ = stats.chi2_contingency(tab, correction=False)\n",
    "\n",
    "print(p_value)\n",
    "    \n",
    "#Check new sampple size\n",
    "tab = pd.crosstab(dataset_df[\"Gender\"], dataset_df[\"label\"])\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now our proportion of females is the same in the SZ and HC groups.\n",
    "\n",
    "Next, let's check for any imbalance with respect to age. The idea is to test the null hypothesis that the  mean (or median) of age between the two group is equal. One way to check this is using the parametric Student's t test for two samples. However, one assumption of Student's t test is that both distributions need to be Gaussian shaped. In order to check if age is normally distributed in each group, we will use the Shapiro-Wilk test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot normal curve\n",
    "ax = sns.kdeplot((data2[data2['label']==0]['Age']), color=\"#839098\", label=(\"HC\"), shade=True)\n",
    "ax = sns.kdeplot((data2[data2['label']==1]['Age']), color=\"#f7d842\", label=(\"SZ\"), shade=True)\n",
    "plt.show()\n",
    "\n",
    "#Shapiro test for normality\n",
    "stat_hc, p_hc = stats.shapiro(data2[data2['label']==0]['Age'])\n",
    "stat_sz, p_sz = stats.shapiro(data2[data2['label']==1]['Age'])\n",
    "\n",
    "def normality(group,stat,p):\n",
    "    print(group)\n",
    "    print('Statistics = %.3f, p = %.3f' % (stat, p))\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('The distribution looks Gaussian')\n",
    "    else:\n",
    "        print('The distribution does not look Gaussian')\n",
    "    print(\"\")\n",
    "    \n",
    "normality(\"HC\",stat_hc, p_hc)\n",
    "normality(\"SZ\",stat_sz, p_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that Age is normally distributed for both groups. The distribution, as well as the meand and standard deviation for each group are fairly similar. However, it is good practice to chack for any staticially significant differences. Therefore, we will go ahead and run the two-sample t-test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptives\n",
    "mean_hc, sd_hc = (data2[data2['label']==0]['Age']).describe().loc[['mean', 'std']]\n",
    "mean_sz, sd_sz = (data2[data2['label']==1]['Age']).describe().loc[['mean', 'std']]\n",
    "\n",
    "\n",
    "age_sz = data2[data2['label']==0]['Age']\n",
    "age_hc = data2[data2['label']==1]['Age']\n",
    "\n",
    "statistic, p_value = stats.ttest_ind(age_sz, age_hc)\n",
    "\n",
    "print('HC: Mean(SD) = %.2f(%.2f)'% (mean_hc, sd_hc))\n",
    "print('SZ: Mean(SD) = %.2f(%.2f)'% (mean_sz, sd_sz))\n",
    "print('t = %.2f, p = %.3f' % (statistic, p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no statsitically significant difference in age between the two groups. Therefore, we will not consider age as a significant confounder in this example.\n",
    "\n",
    "There are several ways to deal with confounding variables. The most obvious and simple one would be to downsample one of the groups until their are similar enough with respect to these variables, like we did for gender. Another popular option would be to regress out the confounding variables from each feature and using the residuals as the new features. For more information on confounding variables in neuroimaging and machine learning learning see Rao et al., 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature set and target\n",
    "\n",
    "The target is a categorical variable that determines whether a participant belongs to the HC and SZ group. Therefore, we will refer to the target variable as labels. This variable corresponds to the column named \"label\" in the dataframe. To create a new variable y with these data, we simply retrieve this column from the dataframe \"data\". Defining the features in the same way would be impractical, since the are too many to be named individually. Instead, we can select a range of columns based on their location. We already know that there are 173 features in the original data. However, not all are brain region volumes; the relevant features for our model start from the 5th column. Therefore, we select the columns of the dataframe accordingly by retrieving a selection of the data containing all rows from the 5th column onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = dataset_df[\"label\"]\n",
    "X = dataset_df[dataset_df.columns[2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the final size of the of our variables, we noticed that we are using 700 subjects in this study. This number is is cleary more that the essential to have to perform a machine learning analysis, conformed analysed here (referencia do parper que diz que precisamos ter 120 sujeitos). However, in many cases we will have a small number of subjects. In these studies, it is always important keep this limitation in mind when discussing the results. This required minimum number of subjects highlight the importance to support the sharing innitiative of the public avaiable datasets, like Neuropen.org (https://openneuro.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FEATURE ENGINEERING\n",
    "\n",
    "In this step, we want to make all the necessary transformations to our data that can help us build a good model. As described in Chapter 2, this can involve different procedures depending on the nature of the data. In this example, we want to use neuroanatomical data to classify SZ and HC.\n",
    "\n",
    "### 3.1. Feature extraction\n",
    "\n",
    "This first step involves extracting brain morphometric information from the raw MRI images. Luckily, this step has already been done for us. The regional grey matter volumes that make up our data X have been extracted with FreeSurfer (REF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Cross-validation\n",
    "\n",
    "Before we move on to apply any transformations to our feature set X, we need to split the data into train and test sets. Recall that this is a critical step to ensure independence between the training and test sets. There are different ways in which to do this. In this example, we will use stratified 10-fold cross-validation (CV). See Chapter 2 for an overview and rational of the most commonly used types of CV. \n",
    "\n",
    "We first transform the dataframe X into a 2D array of the same shape using the library numpy. This will make it easier later on, as some of the functions will require the data to be in this format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values.astype('float32')\n",
    "y = y.values.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare structure to store predictions and coeficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = data[['ID', 'label']].copy()\n",
    "predictions_df['predictions'] = np.nan\n",
    "\n",
    "model_coef_df = pd.DataFrame(columns=data.columns[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the parameters of our stratified CV scheme. For that we will create an object from the class StratifiedKFold from the sklearn library. We will call this object 'skf'. \n",
    "\n",
    "Notice the argument \"random_state\" when we are initializing our object skf. This argument allows us to control the element of randomness intrinsic to splitting the total data into train and test sets. In this example, our data comprises of 740 participants in total. In the code above, we have instructed the model to split it into 10 groups (whilst maintaining the SZ/HC ratio similar throughout the CV iterations). Now, there are multiple solutions to this task! Not setting this hyperparameter to a specific value means that, every time you run your code, the participants assigned to each group will differ! Consequently, your results will, very likely, differ as well*. This is something we would like to avoid, at least while we build upon our model to improve it, as we want to be able to reproduce the same results for comparison between different models.\n",
    "\n",
    "*Importantly to brain disorders research, this is specially true for small sample sizes. For an interesting discussion on the relation between sample size, cross-validation and performance see Varoquax et al. 2018 and Nieuwenhuis et al. 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that the CV is defined, we can loop over each one of the 10 train/test set iterations. At each iteration, we will transform and fit the machine learning algorithm to the training set; and apply the same data transformations and test the algorithm in the test set. \n",
    "\n",
    "We can implement the above using a for loop to iterate over the 10 i_folds. At each i_fold, we will have four new variables:\n",
    " - X_train and y_train: training set and corresponding labels\n",
    " - X_test and y_test: test set and corresponding labels\n",
    "\n",
    "Because we will be training and testing the machine learning algorithm one iteration at a time, we will create some empty variables where we can store important information from each iteration. In the code below we create four empty arrays of shape 10, one for each performance metric: balanced accuracy (bac), sensitivity (sens), specificity (spec) and error rate. Each array will be populated with each metric from each CV iteration. We also created an empty list coefficients. This is where we will store the weights (coefficient or \"importance\") from each feature across the CV iterations. Once the CV is finished, we can then calculate the average weight of each feature for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_bac = np.zeros((n_folds,))\n",
    "cv_test_sens = np.zeros((n_folds,))\n",
    "cv_test_spec = np.zeros((n_folds,))\n",
    "cv_coefficients = []\n",
    "\n",
    "for i_fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many participants there are in the train and test sets in each iteration of the CV. We can do this by simply asking for the length of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\")\n",
    "    print(\"k-fold: \", i_fold + 1)\n",
    "    print(\"N training set:\", len(y_train))\n",
    "    print(\"N test set:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Feature selection \n",
    "\n",
    "Our initial feature set contains 169 features. It is reasonable to assume that some features may be more useful than others for distinguishing SZ patients from HC. Removing less relevant features will speed up the training process and may even improve prediction.\n",
    "\n",
    "This can be done by adding a step known as feature selection to our model. You may recall from Chapter 2 that there are several different ways in which we can implement this step. In this example, we will use a simple univariate approach. Briefly, this approach runs a comparison between the HC and SZ groups for each feature in the training set and selects only a certain amount of features. How the amount of features to keep is determined is up to the researcher. The selected features are then used as input to fit the machine learning model. The same features are then selected from the test set, which are then put through the already trained machine learning model for testing.\n",
    "\n",
    "In this example, we will use the method f_classif. This method runs an ANOVA for each feature and ranks them accoring to their F-statisitc: the larger the F, the larger the difference between the groups. We then proceed to select only the features with an F-statistic in the top 50%. **--- THIS IS VERY RANDOM, I WOULD PREFER TO USE ONLY THE STATISTICAL SIGNIFICANT FEATURES. WHAT DO YOU THINK? I TRIED BUT I COULD ONLY GET THE P-VALUES, I DON'T KNOW HOW TO PASS THEM IN THE SELECTOR..I WILL EXPLAIN THE CODE BELOW BETTER ONCE WE AGREE ON HOW TO SELECT THE FEATURES**\n",
    "\n",
    "There are pleanty of other strategies you could potentially used. You can find out more about other feature selection methods here https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Feature scaling/normalization\n",
    "\n",
    "Before making any transformation to our data, we want to make sure that the fact that different regions of the brain have different ranges will not affect our model. If a feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset. This is likely to cause issues during the model fit.\n",
    "\n",
    "There are several possible solutions to avoid having this issue. In this example, we will transform the data in such a way that the distribution of each feature will resemble a standard normal distribution (e.g. mean=0 and variance=1). Each new normalised value z is calculated by taking each data point xi, subtracting the mean x_ and then dividing it by the standard-deviation sd of the same feature:\n",
    "\n",
    "\n",
    "$$z_{x_i} = \\frac{(\\bar{x}_{feature\\, A} - x_i)}{sd_{feature\\, A}}$$\n",
    "\n",
    "\n",
    "The code below normalises each feature independently using the object StandardScaler from sklearn.\n",
    "\n",
    "First, we create the scaler object. Then, we fit the scaler parameters (mean and standard deviation) using the train set, in other words, we are calculating and storing $\\bar{x}$ and $sd$ in the object scaler. Then, we transform both the train and test set using the formula above with the stored parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train_n = scaler.transform(X_train)\n",
    "    X_test_n = scaler.transform(X_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MODEL TRAINING\n",
    "\n",
    "### 4.1. Machine learning algorithm and hyper-parameter optimization\n",
    "\n",
    "Now that we have the performance metric defined, we can specify our machine learning algorithm. In this example, we will use the popular support vector machine (SVM) as implemented by the sklearn library. You may remember from Chapter 6 that SVM allows the use of different kernels to best separate classes. Here, we will use the default linear kernel for simplicity. The use of linear kernels also make it easier to extract the correficients of the SVM model (feature importance) later on. You can find more information about different kernels at https://scikit-learn.org/stable/modules/svm.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf = LinearSVC(loss='hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, SVM relies on a hyperparameter C that regulates how much we want to avoid misclassifying each training example (see Chapter 6). The ideal method for choosing the value of C is by letting the model try several values and selecting the one with the best performance. This should be done via an additional CV inside the already defined CV, thus creating a nested CV where different values of C are fitted to the training set and tested in the validation set; the value of C with best performance is then used to fit the model to the training set as defined by the outer CV (see Chapter 2 for more details).\n",
    "\n",
    "Fortunately, sklearn has a set of useful tools to implement this. Here, we will use GridSearch, a popular choice in the brain disorders literature. You can find more about GridSearch and other methods for hyperparameter optimisation at https://scikit-learn.org/stable/modules/grid_search.html.\n",
    "\n",
    "To implement GridSearch, we first need to provide a range of possible values for C; this is our search parameter space. Next, we specify the parameters for the GridSearch. We will use stratified kfold again with 10 iterations, as with the outer CV previously defined. The final model design can be seen in Figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Hyperparameter seach space\n",
    "    param_grid = {'C': [2**-6, 2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 2**0, 2**1]}\n",
    "    \n",
    "    #Gridsearch\n",
    "    internal_cv = StratifiedKFold(n_splits=10)\n",
    "    grid_cv = GridSearchCV(estimator=clf, param_grid=param_grid, cv=internal_cv, scoring='balanced_accuracy_score', verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit our SVM model to the training data. We do this by applying the fit command to the features and labels from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    grid_result = grid_cv.fit(X_train_n, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how GridSearch works: we can see the performance for the different values of C in the validation set within the inner CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of C that yields the best performance in the validation set is shown at the top. We then define a second clasifier best_clf, where C takes the best perfomring value and fit it to the training data as defined by the outer CV. Finally, we use this model to make predictions in the test set; these are stored in y_predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_clf = grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. SVM's coefficients\n",
    "\n",
    "In addition to model performance, we are also interested in knowing which features are driving the model's predictions. In the case of the SVM with linear kernel, this feature importance is straightforward to retrieve as they are automatically stored in the parameter .coef_ from our best_clf object.\n",
    "\n",
    "We will store the coefficients for each feature for a particular CV iteration in coefficients_fold. Next, we append these values to the empty list coefficients we have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    coef_abs_value = np.abs(best_clf.coef_.squeeze())\n",
    "    cv_coefficients.append(coef_abs_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MODEL EVALUATION\n",
    "\n",
    "Once the 10 iterations of the CV are finished, we calculate the average of each chosen metric across all iterations. The result will be the final overall performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_predicted = best_clf.predict(X_test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the predicted labels, we can now estimate the performance in the test set. First, we compute the confusion matrix. From here, we estimate balanced accuracy, sensitivity, specificity and error rate. There are plenty more metrics to choose from in sklearn. For a comprehensive list see https://scikit-learn.org/stable/modules/model_evaluation.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Confusion matrix\")\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "    print(\"\")\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    test_bac = balanced_accuracy_score(y_test, y_predicted)\n",
    "    test_sens = tp / (tp + fn)\n",
    "    test_spec = tn / (tn + fp)\n",
    "\n",
    "    print(\"Balanced accuracy: %.4f \" % (test_bac))\n",
    "    print(\"Sensitivity: %.4f \" % (test_sens))\n",
    "    print(\"Specificity: %.4f \" % (test_spec))\n",
    "\n",
    "    cv_test_bac[i_fold] = test_bac\n",
    "    cv_test_sens[i_fold] = test_sens\n",
    "    cv_test_spec[i_fold] = test_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross-validation Balanced accuracy: %.4f +- %.4f\" % (cv_test_bac.mean(), cv_test_bac.std()))\n",
    "print(\"Cross-validation Sensitivity: %.4f +- %.4f\" % (cv_test_sens.mean(), cv_test_sens.std()))\n",
    "print(\"Cross-validation Specificity: %.4f +- %.4f\" % (cv_test_spec.mean(), cv_test_spec.std()))\n",
    "print(\"Cross-validation Error Rate: %.4f +- %.4f\" % (cv_error_rate.mean(), cv_error_rate.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to save your main results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving feature importance\n",
    "model_coef_df.to_csv(\"./results/\" + experiment_name + \"/feature_importance.csv\")\n",
    "\n",
    "# Saving predictions\n",
    "predictions_df.to_csv(\"./results/\" + experiment_name + \"/predictions.csv\")\n",
    "\n",
    "# Saving BAC\n",
    "bac_filename = \"./results/\" + experiment_name + \"/final_BAC.npy\"\n",
    "np.save(bac_filename, cv_test_bac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Save the model's predictions\n",
    "\n",
    "Let's save the true labels, the predicted labels and the trained model for each CV iteration. This informtion may come in handy later on for some additional analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, value in zip(test_index, y_predicted):\n",
    "    predictions_df.at[row, 'predictions'] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. POST-HOC ANALYSIS\n",
    "\n",
    "Once we have our final model, we can several additional analysis. Here, we will run the following analyses:\n",
    " - Test balanced accuracy for statistical significance via permutation testing \n",
    " - Identify the features that contributed the most for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Significance test and most informative features\n",
    "\n",
    "EXPLAIN CODE HERE IF THIS IS THE FINAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = np.mean((np.asarray(cv_coefficients)), axis=0)\n",
    "#coefficients = coefficients[(-coefficients).argsort()[:15]]\n",
    "feature_names = data.iloc[:,4:].columns\n",
    "coefficients = pd.DataFrame(data=coefficients).transpose()\n",
    "coefficients.columns=feature_names\n",
    "ax = coefficients.plot(kind='barh', figsize=(8, 10), legend=False)  # plot weights\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot above shows the most important features to classify a participant as a patient \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
