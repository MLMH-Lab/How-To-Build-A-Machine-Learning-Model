{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19. Tutorial\n",
    "\n",
    "In this tutorial, we will implement a complete machine learning pipeline. The aim of this code is to show the facility and practicality to implement your pipelines in Python.\n",
    "\n",
    "Ok. Before we even start to implement our pipeline, it is necessary to perform some previous steps in our code.\n",
    "\n",
    "recorte\n",
    "This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.\n",
    "\n",
    "Following is the code to do this.\n",
    "Let’s start by getting your hands on the data. \n",
    "\n",
    "The Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made available by Kaggle as part of a computer-vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t already have one—don’t worry, the process is painless)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing helper libraries\n",
    "\n",
    "It is good practice to begin the code file by importing all the libraries you will need in your analysis. In this tutorial, we will use the following main libraries:\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - matplotlib and seaborn\n",
    "    - scikit-learn\n",
    "    \n",
    "To make the code clearer, it is common to import certain libraries and assigning it a nickname. For example, the  pandas library is typically imported as **pd**. This way, we simply type pd every time we want to call the pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warnings\n",
    "import warnings\n",
    "\n",
    "#Store and organize output files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#Manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Statistical tests\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "#Machine learning\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "# Ignore WARNING\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed\n",
    "\n",
    "In our analysis, there will be some elements of randomness. For example, we might want to remove some participants at random during data cleaning. Likewise, when defining the cross-validation scheme (CV), the train/test partition at each iteration is also done at random. In Python, this randomness can be controlled by setting a \"seed value\" to a fixed value. Not defining a specific seed value means that the variables that rely on this element of randomness will have a different value every time you run your code. For example, the division of participants between the train and test sets will be different, which will likely lead to a different result.\n",
    "\n",
    "Setting the seed value guarantees that you will get the same results every time you run your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "np.random.seed = random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize your workspace\n",
    "\n",
    "First, we will create  the structure of flder where you can store all your results. Along your projects, you might test different strategies to perform the machine learning pipeline. As we mentioned before, the machine learning analysis is performed by several try and errors (e.g., try different models, try different preprocessing, etc.). After a lot of testing, it will be easy to feel lost. So, give your experiment a name, create a folder in the results directory with the same name, and store your experiment outputs is this local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('./results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "experiment_name = 'linear_SVM_example'\n",
    "experiment_dir = results_dir / experiment_name\n",
    "experiment_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem formulation\n",
    "With your enviroment set, we can perform the first step in your machine learning pipeline: *the definition of our machine learning problem*.\n",
    "\n",
    "Usually, this step is performed outside even before we start to write the source code. The goals of our analysis should be discussed when defining our research question, when you are defining your project scope. This will help to orient what you will do and not do, which data we should collect, and what kind of analysis we will perform. In this tutorial, our machine learning problem is: \n",
    "\n",
    "*We will use structural MRI data to classify subjects in two groups: healthy control and patients with schizophrenia.*\n",
    "\n",
    "**Consider add a Figure very high level showing the problem**\n",
    "\n",
    "In the previous sentence, we can find all the importants elements when defining a machine learning problem. We have:\n",
    "\n",
    "- **Features**: Structural MRI data\n",
    "- **Task**: Classification\n",
    "- **Target**: Patients with schizophrenia and Healthy Control\n",
    "\n",
    "Having your problem well framed is essential when conducting any kind of project, and it make easier to share the objectives of your experiments with other team members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "In this step, we will start getting your hands on the data. The idea is to perform a series of analysis to get our data ready for analyse it with the machine learning models. For this reason, the first steps will involve reading the data and performing a lot of exploratory analysis. Different analyses should be involved depending on the nature of your problem and the type of your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loading the data\n",
    "\n",
    "In this example, we will use tabular data with columns as the features, targets and demographic data, and rows as participants. The data are stored as a Comma-Separated Values (CSV) file. We will use the library pandas to load and explore the data. \n",
    "\n",
    "From pandas, we use the function read_csv() to load the csv file containing our data. This will load the data into an object type called dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decide how we load the data in the text version of the book: 1) provide a download link, 2) run code to download it automatically into a specific directory using !wget command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(\"./Chapter_19_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by seeing the first six rows of the data. Selecting subsections (\"slicing\") of a dataframe using pandas is straighforward. There are different ways to do this. Here, we simply retrieve the six rows from the dataframe data (note that the first column is indexed 0 and the last column is not included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Left_Lateral_Ventricle</th>\n",
       "      <th>Left_Inf_Lat_Vent</th>\n",
       "      <th>Left_Cerebellum_White_Matter</th>\n",
       "      <th>Left_Cerebellum_Cortex</th>\n",
       "      <th>Left_Thalamus_Proper</th>\n",
       "      <th>Left_Caudate</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_rostralanteriorcingulate_thickness</th>\n",
       "      <th>rh_rostralmiddlefrontal_thickness</th>\n",
       "      <th>rh_superiorfrontal_thickness</th>\n",
       "      <th>rh_superiorparietal_thickness</th>\n",
       "      <th>rh_superiortemporal_thickness</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>4226.907844</td>\n",
       "      <td>414.407845</td>\n",
       "      <td>12242.90784</td>\n",
       "      <td>43410.50784</td>\n",
       "      <td>7020.107844</td>\n",
       "      <td>4133.407844</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440844</td>\n",
       "      <td>2.522844</td>\n",
       "      <td>2.656844</td>\n",
       "      <td>2.123844</td>\n",
       "      <td>2.638844</td>\n",
       "      <td>2.420844</td>\n",
       "      <td>2.489844</td>\n",
       "      <td>2.235844</td>\n",
       "      <td>2.300844</td>\n",
       "      <td>2.645844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c002</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>4954.912699</td>\n",
       "      <td>414.812699</td>\n",
       "      <td>16519.51270</td>\n",
       "      <td>38808.31270</td>\n",
       "      <td>7013.312699</td>\n",
       "      <td>3882.912699</td>\n",
       "      <td>...</td>\n",
       "      <td>2.507699</td>\n",
       "      <td>2.470699</td>\n",
       "      <td>2.645699</td>\n",
       "      <td>2.132699</td>\n",
       "      <td>2.848699</td>\n",
       "      <td>2.425699</td>\n",
       "      <td>2.883699</td>\n",
       "      <td>2.622699</td>\n",
       "      <td>2.322699</td>\n",
       "      <td>2.673699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c003</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>4470.611989</td>\n",
       "      <td>370.111989</td>\n",
       "      <td>10193.51199</td>\n",
       "      <td>38637.51199</td>\n",
       "      <td>5802.911989</td>\n",
       "      <td>2941.711989</td>\n",
       "      <td>...</td>\n",
       "      <td>2.545989</td>\n",
       "      <td>2.589989</td>\n",
       "      <td>2.885989</td>\n",
       "      <td>2.317989</td>\n",
       "      <td>2.326989</td>\n",
       "      <td>2.454989</td>\n",
       "      <td>2.482989</td>\n",
       "      <td>2.232989</td>\n",
       "      <td>2.267989</td>\n",
       "      <td>2.795989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>7553.310654</td>\n",
       "      <td>521.010654</td>\n",
       "      <td>12716.01065</td>\n",
       "      <td>41933.31065</td>\n",
       "      <td>5998.310654</td>\n",
       "      <td>2869.110654</td>\n",
       "      <td>...</td>\n",
       "      <td>2.323654</td>\n",
       "      <td>2.411654</td>\n",
       "      <td>2.770654</td>\n",
       "      <td>2.149654</td>\n",
       "      <td>2.458654</td>\n",
       "      <td>2.307654</td>\n",
       "      <td>3.284654</td>\n",
       "      <td>1.956654</td>\n",
       "      <td>2.297654</td>\n",
       "      <td>2.731654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>8785.212771</td>\n",
       "      <td>396.912771</td>\n",
       "      <td>12077.41277</td>\n",
       "      <td>41818.91277</td>\n",
       "      <td>5839.812771</td>\n",
       "      <td>3614.812771</td>\n",
       "      <td>...</td>\n",
       "      <td>3.211771</td>\n",
       "      <td>2.467771</td>\n",
       "      <td>2.772771</td>\n",
       "      <td>2.051771</td>\n",
       "      <td>2.588771</td>\n",
       "      <td>2.325771</td>\n",
       "      <td>3.266771</td>\n",
       "      <td>3.162771</td>\n",
       "      <td>2.081771</td>\n",
       "      <td>2.607771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c006</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>5083.706643</td>\n",
       "      <td>172.106643</td>\n",
       "      <td>11927.50664</td>\n",
       "      <td>38730.80664</td>\n",
       "      <td>5693.506643</td>\n",
       "      <td>3422.606643</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562643</td>\n",
       "      <td>2.603643</td>\n",
       "      <td>2.948643</td>\n",
       "      <td>2.177643</td>\n",
       "      <td>2.489643</td>\n",
       "      <td>2.362643</td>\n",
       "      <td>2.314643</td>\n",
       "      <td>3.512643</td>\n",
       "      <td>2.591643</td>\n",
       "      <td>2.606643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  label  Gender  Age  Left_Lateral_Ventricle  Left_Inf_Lat_Vent  \\\n",
       "0  c001      0       0   35             4226.907844         414.407845   \n",
       "1  c002      0       1   37             4954.912699         414.812699   \n",
       "2  c003      0       1   32             4470.611989         370.111989   \n",
       "3  c004      0       1   36             7553.310654         521.010654   \n",
       "4  c005      0       0   22             8785.212771         396.912771   \n",
       "5  c006      0       1   34             5083.706643         172.106643   \n",
       "\n",
       "   Left_Cerebellum_White_Matter  Left_Cerebellum_Cortex  Left_Thalamus_Proper  \\\n",
       "0                   12242.90784             43410.50784           7020.107844   \n",
       "1                   16519.51270             38808.31270           7013.312699   \n",
       "2                   10193.51199             38637.51199           5802.911989   \n",
       "3                   12716.01065             41933.31065           5998.310654   \n",
       "4                   12077.41277             41818.91277           5839.812771   \n",
       "5                   11927.50664             38730.80664           5693.506643   \n",
       "\n",
       "   Left_Caudate         ...           rh_rostralanteriorcingulate_thickness  \\\n",
       "0   4133.407844         ...                                        2.440844   \n",
       "1   3882.912699         ...                                        2.507699   \n",
       "2   2941.711989         ...                                        2.545989   \n",
       "3   2869.110654         ...                                        2.323654   \n",
       "4   3614.812771         ...                                        3.211771   \n",
       "5   3422.606643         ...                                        2.562643   \n",
       "\n",
       "   rh_rostralmiddlefrontal_thickness  rh_superiorfrontal_thickness  \\\n",
       "0                           2.522844                      2.656844   \n",
       "1                           2.470699                      2.645699   \n",
       "2                           2.589989                      2.885989   \n",
       "3                           2.411654                      2.770654   \n",
       "4                           2.467771                      2.772771   \n",
       "5                           2.603643                      2.948643   \n",
       "\n",
       "   rh_superiorparietal_thickness  rh_superiortemporal_thickness  \\\n",
       "0                       2.123844                       2.638844   \n",
       "1                       2.132699                       2.848699   \n",
       "2                       2.317989                       2.326989   \n",
       "3                       2.149654                       2.458654   \n",
       "4                       2.051771                       2.588771   \n",
       "5                       2.177643                       2.489643   \n",
       "\n",
       "   rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "0                    2.420844                  2.489844   \n",
       "1                    2.425699                  2.883699   \n",
       "2                    2.454989                  2.482989   \n",
       "3                    2.307654                  3.284654   \n",
       "4                    2.325771                  3.266771   \n",
       "5                    2.362643                  2.314643   \n",
       "\n",
       "   rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "0                   2.235844                         2.300844   \n",
       "1                   2.622699                         2.322699   \n",
       "2                   2.232989                         2.267989   \n",
       "3                   1.956654                         2.297654   \n",
       "4                   3.162771                         2.081771   \n",
       "5                   3.512643                         2.591643   \n",
       "\n",
       "   rh_insula_thickness  \n",
       "0             2.645844  \n",
       "1             2.673699  \n",
       "2             2.795989  \n",
       "3             2.731654  \n",
       "4             2.607771  \n",
       "5             2.606643  \n",
       "\n",
       "[6 rows x 173 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> dataset_df[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the content of the dataset_df variable, we can see the column names at the top and the contents of the first six participants. The columns include the ID, age and gender, as well as the gray matter volume for several brain regions. We can see that there is at least one value missing. We will deal with this later. \n",
    "\n",
    "It may be useful to know all the features that are avaiable in the dataset. To do this, we simply ask for the names of the columns of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'label',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Left_Lateral_Ventricle',\n",
       " 'Left_Inf_Lat_Vent',\n",
       " 'Left_Cerebellum_White_Matter',\n",
       " 'Left_Cerebellum_Cortex',\n",
       " 'Left_Thalamus_Proper',\n",
       " 'Left_Caudate',\n",
       " 'Left_Putamen',\n",
       " 'Left_Pallidum',\n",
       " 'rd_Ventricle',\n",
       " 'th_Ventricle',\n",
       " 'Brain_Stem',\n",
       " 'Left_Hippocampus',\n",
       " 'Left_Amygdala',\n",
       " 'CSF',\n",
       " 'Left_Accumbens_area',\n",
       " 'Left_VentralDC',\n",
       " 'Right_Lateral_Ventricle',\n",
       " 'Right_Inf_Lat_Vent',\n",
       " 'Right_Cerebellum_White_Matter',\n",
       " 'Right_Cerebellum_Cortex',\n",
       " 'Right_Thalamus_Proper',\n",
       " 'Right_Caudate',\n",
       " 'Right_Putamen',\n",
       " 'Right_Pallidum',\n",
       " 'Right_Hippocampus',\n",
       " 'Right_Amygdala',\n",
       " 'Right_Accumbens_area',\n",
       " 'Right_VentralDC',\n",
       " 'CC_Posterior',\n",
       " 'CC_Mid_Posterior',\n",
       " 'CC_Central',\n",
       " 'CC_Mid_Anterior',\n",
       " 'CC_Anterior',\n",
       " 'lh_bankssts_volume',\n",
       " 'lh_caudalanteriorcingulate_volume',\n",
       " 'lh_caudalmiddlefrontal_volume',\n",
       " 'lh_cuneus_volume',\n",
       " 'lh_entorhinal_volume',\n",
       " 'lh_fusiform_volume',\n",
       " 'lh_inferiorparietal_volume',\n",
       " 'lh_inferiortemporal_volume',\n",
       " 'lh_isthmuscingulate_volume',\n",
       " 'lh_lateraloccipital_volume',\n",
       " 'lh_lateralorbitofrontal_volume',\n",
       " 'lh_lingual_volume',\n",
       " 'lh_medialorbitofrontal_volume',\n",
       " 'lh_middletemporal_volume',\n",
       " 'lh_parahippocampal_volume',\n",
       " 'lh_paracentral_volume',\n",
       " 'lh_parsopercularis_volume',\n",
       " 'lh_parsorbitalis_volume',\n",
       " 'lh_parstriangularis_volume',\n",
       " 'lh_pericalcarine_volume',\n",
       " 'lh_postcentral_volume',\n",
       " 'lh_posteriorcingulate_volume',\n",
       " 'lh_precentral_volume',\n",
       " 'lh_precuneus_volume',\n",
       " 'lh_rostralanteriorcingulate_volume',\n",
       " 'lh_rostralmiddlefrontal_volume',\n",
       " 'lh_superiorfrontal_volume',\n",
       " 'lh_superiorparietal_volume',\n",
       " 'lh_superiortemporal_volume',\n",
       " 'lh_supramarginal_volume',\n",
       " 'lh_frontalpole_volume',\n",
       " 'lh_temporalpole_volume',\n",
       " 'lh_transversetemporal_volume',\n",
       " 'lh_insula_volume',\n",
       " 'rh_bankssts_volume',\n",
       " 'rh_caudalanteriorcingulate_volume',\n",
       " 'rh_caudalmiddlefrontal_volume',\n",
       " 'rh_cuneus_volume',\n",
       " 'rh_entorhinal_volume',\n",
       " 'rh_fusiform_volume',\n",
       " 'rh_inferiorparietal_volume',\n",
       " 'rh_inferiortemporal_volume',\n",
       " 'rh_isthmuscingulate_volume',\n",
       " 'rh_lateraloccipital_volume',\n",
       " 'rh_lateralorbitofrontal_volume',\n",
       " 'rh_lingual_volume',\n",
       " 'rh_medialorbitofrontal_volume',\n",
       " 'rh_middletemporal_volume',\n",
       " 'rh_parahippocampal_volume',\n",
       " 'rh_paracentral_volume',\n",
       " 'rh_parsopercularis_volume',\n",
       " 'rh_parsorbitalis_volume',\n",
       " 'rh_parstriangularis_volume',\n",
       " 'rh_pericalcarine_volume',\n",
       " 'rh_postcentral_volume',\n",
       " 'rh_posteriorcingulate_volume',\n",
       " 'rh_precentral_volume',\n",
       " 'rh_precuneus_volume',\n",
       " 'rh_rostralanteriorcingulate_volume',\n",
       " 'rh_rostralmiddlefrontal_volume',\n",
       " 'rh_superiorfrontal_volume',\n",
       " 'rh_superiorparietal_volume',\n",
       " 'rh_superiortemporal_volume',\n",
       " 'rh_supramarginal_volume',\n",
       " 'rh_frontalpole_volume',\n",
       " 'rh_temporalpole_volume',\n",
       " 'rh_transversetemporal_volume',\n",
       " 'rh_insula_volume',\n",
       " 'lh_bankssts_thickness',\n",
       " 'lh_caudalanteriorcingulate_thickness',\n",
       " 'lh_caudalmiddlefrontal_thickness',\n",
       " 'lh_cuneus_thickness',\n",
       " 'lh_entorhinal_thickness',\n",
       " 'lh_fusiform_thickness',\n",
       " 'lh_inferiorparietal_thickness',\n",
       " 'lh_inferiortemporal_thickness',\n",
       " 'lh_isthmuscingulate_thickness',\n",
       " 'lh_lateraloccipital_thickness',\n",
       " 'lh_lateralorbitofrontal_thickness',\n",
       " 'lh_lingual_thickness',\n",
       " 'lh_medialorbitofrontal_thickness',\n",
       " 'lh_middletemporal_thickness',\n",
       " 'lh_parahippocampal_thickness',\n",
       " 'lh_paracentral_thickness',\n",
       " 'lh_parsopercularis_thickness',\n",
       " 'lh_parsorbitalis_thickness',\n",
       " 'lh_parstriangularis_thickness',\n",
       " 'lh_pericalcarine_thickness',\n",
       " 'lh_postcentral_thickness',\n",
       " 'lh_posteriorcingulate_thickness',\n",
       " 'lh_precentral_thickness',\n",
       " 'lh_precuneus_thickness',\n",
       " 'lh_rostralanteriorcingulate_thickness',\n",
       " 'lh_rostralmiddlefrontal_thickness',\n",
       " 'lh_superiorfrontal_thickness',\n",
       " 'lh_superiorparietal_thickness',\n",
       " 'lh_superiortemporal_thickness',\n",
       " 'lh_supramarginal_thickness',\n",
       " 'lh_frontalpole_thickness',\n",
       " 'lh_temporalpole_thickness',\n",
       " 'lh_transversetemporal_thickness',\n",
       " 'lh_insula_thickness',\n",
       " 'rh_bankssts_thickness',\n",
       " 'rh_caudalanteriorcingulate_thickness',\n",
       " 'rh_caudalmiddlefrontal_thickness',\n",
       " 'rh_cuneus_thickness',\n",
       " 'rh_entorhinal_thickness',\n",
       " 'rh_fusiform_thickness',\n",
       " 'rh_inferiorparietal_thickness',\n",
       " 'rh_inferiortemporal_thickness',\n",
       " 'rh_isthmuscingulate_thickness',\n",
       " 'rh_lateraloccipital_thickness',\n",
       " 'rh_lateralorbitofrontal_thickness',\n",
       " 'rh_lingual_thickness',\n",
       " 'rh_medialorbitofrontal_thickness',\n",
       " 'rh_middletemporal_thickness',\n",
       " 'rh_parahippocampal_thickness',\n",
       " 'rh_paracentral_thickness',\n",
       " 'rh_parsopercularis_thickness',\n",
       " 'rh_parsorbitalis_thickness',\n",
       " 'rh_parstriangularis_thickness',\n",
       " 'rh_pericalcarine_thickness',\n",
       " 'rh_postcentral_thickness',\n",
       " 'rh_posteriorcingulate_thickness',\n",
       " 'rh_precentral_thickness',\n",
       " 'rh_precuneus_thickness',\n",
       " 'rh_rostralanteriorcingulate_thickness',\n",
       " 'rh_rostralmiddlefrontal_thickness',\n",
       " 'rh_superiorfrontal_thickness',\n",
       " 'rh_superiorparietal_thickness',\n",
       " 'rh_superiortemporal_thickness',\n",
       " 'rh_supramarginal_thickness',\n",
       " 'rh_frontalpole_thickness',\n",
       " 'rh_temporalpole_thickness',\n",
       " 'rh_transversetemporal_thickness',\n",
       " 'rh_insula_thickness']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what features we are dealing with, let's check the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features = 173\n",
      "Number of participants = 740\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features =\", dataset_df.shape[1])\n",
    "print(\"Number of participants =\", dataset_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, our data preparation stage will check the dataset for the presence of:\n",
    " - Missing data \n",
    " - Data imbalance with respect to the labels\n",
    " - Confounding variables\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Missing data\n",
    "\n",
    "Many machine learning algorithms do not support data with missing values. Therefore, it is important to check if there are any missing values in our data. There are many different ways to do this. Here we will build our own function to loop through each the column in the dataframe data (note data we are also including gender and age) and get the feature name and Id for the corresponding missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAFAEL DO REVIEW THIS FUNCTION\n",
    "\n",
    "def detect_nan(dataset):\n",
    "    nan_total = dataset.isnull().sum().sum() \n",
    "    if nan_total > 0:\n",
    "        for column in dataset:          \n",
    "            #Find Ids with nan - THIS IS PROBABLY OVERLY COMPLICATED (all I want here is to get the Ids of where the nans are so I can print them later on)\n",
    "            nan = dataset[column].isnull()\n",
    "            dataset[\"nan\"] = nan \n",
    "            ids = []\n",
    "            for i in dataset[\"nan\"]:\n",
    "                if i == True:\n",
    "                    id_nan = dataset.loc[dataset[\"nan\"] == True, 'ID']\n",
    "                    ids.append(id_nan)               \n",
    "            #Calculate total number of nan for each feature and Id\n",
    "            nan_sum = nan.sum()          \n",
    "            if nan_sum > 0:\n",
    "                print(\"Found\", nan_sum, \"missing value(s) for\", column, \"for Id(s):\", *ids[0])             \n",
    "        #dataset = dataset.drop(columns=[\"nan\"])\n",
    "    else:\n",
    "        print(\"There are no missing data in this dataset!\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing data in this dataset!\n",
      "\n"
     ]
    }
   ],
   "source": [
    ">>> detect_nan(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can check that there are 12 missing values for gender and age. There are several options to go from here with different degrees of complexity (see Chapter x for a more in-depth description). In this example, we will simply remove the participants with missing data. Not having this information does not allow for a thorough assessment of imbalanced demographic data, which could be problematic. There are sophisticated options to input missing data however they all come with disadvantages. Since removing these participants would only result in losing x% of the total data, this option wil not result in a large drop in sample size. We can remove missing data using the funciton .dropna() from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants = 740\n"
     ]
    }
   ],
   "source": [
    "dataset_df = dataset_df.dropna()\n",
    "\n",
    "print(\"Number of participants =\", dataset_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the new dataframe has now x less participants than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Class imbalance\n",
    "\n",
    "Next, let's check the number of total participants in each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients with schizophrenia (SZ) =\", len(y[y == 0]))\n",
    "print(\"Number of Healthy Control (HC) =\", len(y[y == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that there are 740 participants in total, 368 patients and 372 controls. There does not seem to be a large imbalance between classes. However, they are not perfectly matched. One option would be to downsample the HC to match the SZ group. However, this would mean losing some data. Since the imbalance is not too large, we will use balanced accuracy as our metric of choice as well as a stratified CV to ensure the same proportion SZ/HC across the CV iterations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Confounding variables\n",
    "\n",
    "Next, let's check for balance of some obvious confounding variables: gender and age. First, let's see the distribution of gender between the two classes. This time, we will use some plots from the seaborn library. Plotting data using seaborn is straighforward. For more information see https://seaborn.pydata.org. Seaborn operates under matplotlib, the most widely used plotting library. Threfore, we will also import it. You can find out more about matplotlib here https://matplotlib.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"label\", hue=\"Gender\", data=data, palette=['#839098', '#f7d842'])\n",
    "\n",
    "plt.legend([\"Male\", \"Female\"])\n",
    "plt.xticks([0,1], [\"HC\", \"SZ\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a fairly similar number of males in the two groups. However, the control group has more females than the patient group. Let's run a Chi-squared test to check if this difference is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.crosstab(data[\"Gender\"], data[\"label\"])\n",
    "print(tab)\n",
    "print(\"\")\n",
    "\n",
    "chi2, p_value, _, _ = stats.chi2_contingency(tab, correction=False)\n",
    "print('chi2 = %.3f' % chi2)  \n",
    "print('p = %.3f' % p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that there is indeed a statistically significant difference between the two classes with respect to gender. This may be an issue, as the impact of gender on brain morphology has been well-established. Therefore, the algorithm may use brain features that are associated with gender differences to distinguish between HC and SZ, as opposed to differences related to the disorder. - CONFIRM THIS IS WELL EXPLAINED\n",
    "\n",
    "To mitigate the bias this imabalance will introduce in our model, we will downsample the number of females in the HC class by randomly selecting and removing the necessary amount of participants to match the number of females in the SZ group. From the output above we can see that there are 163 and 120 females in the HC and SZ groups, respectivelly. Therefore, we will remove 42 females from the HC group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select 42 female controls at random and get their indexes\n",
    "to_remove = (data[(data['label']==0) & (data['Gender'] == 1)]).sample(n=42, random_state=1).index\n",
    "\n",
    "#remove them from the data\n",
    "data2 = data.drop(to_remove)\n",
    "\n",
    "#Check new sampple size\n",
    "tab = pd.crosstab(data2[\"Gender\"], data2[\"label\"])\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are now the same number of females in the SZ and HC groups.\n",
    "\n",
    "Next, let's check for any imbalance with respect to age by running a two-sample t-test between HC and SZ. Recall a t-test assumes the data are normally distributed. Therefore, we will first check is age is normally distributed in each group. We do this by inspecting the distribution for Age for the groups, checking the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot normal curve\n",
    "ax = sns.kdeplot((data2[data2['label']==0]['Age']), color=\"#839098\", label=(\"HC\"), shade=True)\n",
    "ax = sns.kdeplot((data2[data2['label']==1]['Age']), color=\"#f7d842\", label=(\"SZ\"), shade=True)\n",
    "plt.show()\n",
    "\n",
    "#Shapiro test for normality\n",
    "stat_hc, p_hc = shapiro(data2[data2['label']==0]['Age'])\n",
    "stat_sz, p_sz = shapiro(data2[data2['label']==1]['Age'])\n",
    "\n",
    "def normality(group,stat,p):\n",
    "    print(group)\n",
    "    print('Statistics = %.3f, p = %.3f' % (stat, p))\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('The distribution looks Gaussian')\n",
    "    else:\n",
    "        print('The distribution does not look Gaussian')\n",
    "    print(\"\")\n",
    "    \n",
    "normality(\"HC\",stat_hc, p_hc)\n",
    "normality(\"SZ\",stat_sz, p_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that Age is normally distributed for both groups. The distribution, as well as the meand and standard deviation for each group are fairly similar. However, it is good practice to chack for any staticially significant differences. Therefore, we will go ahead and run the two-sample t-test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptives\n",
    "mean_hc, sd_hc = (data2[data2['label']==0]['Age']).describe().loc[['mean', 'std']]\n",
    "mean_sz, sd_sz = (data2[data2['label']==1]['Age']).describe().loc[['mean', 'std']]\n",
    "\n",
    "\n",
    "age_sz = data2[data2['label']==0]['Age']\n",
    "age_hc = data2[data2['label']==1]['Age']\n",
    "\n",
    "statistic, p_value = stats.ttest_ind(age_sz, age_hc)\n",
    "\n",
    "print('HC: Mean(SD) = %.2f(%.2f)'% (mean_hc, sd_hc))\n",
    "print('SZ: Mean(SD) = %.2f(%.2f)'% (mean_sz, sd_sz))\n",
    "print('t = %.2f, p = %.3f' % (statistic, p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no statsitically significant difference in age between the two groups. Therefore, we will not consider age as a significant confounder in this example.\n",
    "\n",
    "There are several ways to deal with confounding variables. The most obvious and simple one would be to downsample one of the groups until their are similar enough with respect to these variables, like we did for gender. Another popular option would be to regress out the confounding variables from each feature and using the residuals as the new features. For more information on confounding variables in neuroimaging and machine learning learning see Rao et al., 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature set and target\n",
    "\n",
    "The target is a categorical variable that determines whether a participant belongs to the HC and SZ group. Therefore, we will refer to the target variable as labels. This variable corresponds to the column named \"label\" in the dataframe. To create a new variable y with these data, we simply retrieve this column from the dataframe \"data\". Defining the features in the same way would be impractical, since the are too many to be named individually. Instead, we can select a range of columns based on their location. We already know that there are 173 features in the original data. However, not all are brain region volumes; the relevant features for our model start from the 5th column. Therefore, we select the columns of the dataframe accordingly by retrieving a selection of the data containing all rows from the 5th column onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = dataset_df[\"label\"]\n",
    "X = dataset_df[dataset_df.columns[2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FEATURE ENGINEERING\n",
    "\n",
    "In this step, we want to make all the necessary transformations to our data that can help us build a good model. As described in Chapter 2, this can involve different procedures depending on the nature of the data. In this example, we want to use neuroanatomical data to classify SZ and HC.\n",
    "\n",
    "### 3.1. Feature extraction\n",
    "\n",
    "This first step involves extracting brain morphometric information from the raw MRI images. Luckily, this step has already been done for us. The regional grey matter volumes that make up our data X have been extracted with FreeSurfer (REF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Cross-validation\n",
    "\n",
    "Before we move on to apply any transformations to our feature set X, we need to split the data into train and test sets. Recall that this is a critical step to ensure independence between the training and test sets. There are different ways in which to do this. In this example, we will use stratified 10-fold cross-validation (CV). See Chapter 2 for an overview and rational of the most commonly used types of CV. \n",
    "\n",
    "We first transform the dataframe X into a 2D array of the same shape using the library numpy. This will make it easier later on, as some of the functions will require the data to be in this format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values.astype('float32')\n",
    "y = y.values.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare structure to store predictions and coeficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = data[['ID', 'label']].copy()\n",
    "predictions_df['predictions'] = np.nan\n",
    "\n",
    "model_coef_df = pd.DataFrame(columns=data.columns[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the parameters of our stratified CV scheme. For that we will create an object from the class StratifiedKFold from the sklearn library. We will call this object 'skf'. \n",
    "\n",
    "Notice the argument \"random_state\" when we are initializing our object skf. This argument allows us to control the element of randomness intrinsic to splitting the total data into train and test sets. In this example, our data comprises of 740 participants in total. In the code above, we have instructed the model to split it into 10 groups (whilst maintaining the SZ/HC ratio similar throughout the CV iterations). Now, there are multiple solutions to this task! Not setting this hyperparameter to a specific value means that, every time you run your code, the participants assigned to each group will differ! Consequently, your results will, very likely, differ as well*. This is something we would like to avoid, at least while we build upon our model to improve it, as we want to be able to reproduce the same results for comparison between different models.\n",
    "\n",
    "*Importantly to brain disorders research, this is specially true for small sample sizes. For an interesting discussion on the relation between sample size, cross-validation and performance see Varoquax et al. 2018 and Nieuwenhuis et al. 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that the CV is defined, we can loop over each one of the 10 train/test set iterations. At each iteration, we will transform and fit the machine learning algorithm to the training set; and apply the same data transformations and test the algorithm in the test set. \n",
    "\n",
    "We can implement the above using a for loop to iterate over the 10 i_folds. At each i_fold, we will have four new variables:\n",
    " - X_train and y_train: training set and corresponding labels\n",
    " - X_test and y_test: test set and corresponding labels\n",
    "\n",
    "Because we will be training and testing the machine learning algorithm one iteration at a time, we will create some empty variables where we can store important information from each iteration. In the code below we create four empty arrays of shape 10, one for each performance metric: balanced accuracy (bac), sensitivity (sens), specificity (spec) and error rate. Each array will be populated with each metric from each CV iteration. We also created an empty list coefficients. This is where we will store the weights (coefficient or \"importance\") from each feature across the CV iterations. Once the CV is finished, we can then calculate the average weight of each feature for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_bac = np.zeros((n_folds,))\n",
    "cv_test_sens = np.zeros((n_folds,))\n",
    "cv_test_spec = np.zeros((n_folds,))\n",
    "cv_coefficients = []\n",
    "\n",
    "for i_fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many participants there are in the train and test sets in each iteration of the CV. We can do this by simply asking for the length of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\")\n",
    "    print(\"k-fold: \", i_fold + 1)\n",
    "    print(\"N training set:\", len(y_train))\n",
    "    print(\"N test set:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Feature selection \n",
    "\n",
    "Our initial feature set contains 169 features. It is reasonable to assume that some features may be more useful than others for distinguishing SZ patients from HC. Removing less relevant features will speed up the training process and may even improve prediction.\n",
    "\n",
    "This can be done by adding a step known as feature selection to our model. You may recall from Chapter 2 that there are several different ways in which we can implement this step. In this example, we will use a simple univariate approach. Briefly, this approach runs a comparison between the HC and SZ groups for each feature in the training set and selects only a certain amount of features. How the amount of features to keep is determined is up to the researcher. The selected features are then used as input to fit the machine learning model. The same features are then selected from the test set, which are then put through the already trained machine learning model for testing.\n",
    "\n",
    "In this example, we will use the method f_classif. This method runs an ANOVA for each feature and ranks them accoring to their F-statisitc: the larger the F, the larger the difference between the groups. We then proceed to select only the features with an F-statistic in the top 50%. **--- THIS IS VERY RANDOM, I WOULD PREFER TO USE ONLY THE STATISTICAL SIGNIFICANT FEATURES. WHAT DO YOU THINK? I TRIED BUT I COULD ONLY GET THE P-VALUES, I DON'T KNOW HOW TO PASS THEM IN THE SELECTOR..I WILL EXPLAIN THE CODE BELOW BETTER ONCE WE AGREE ON HOW TO SELECT THE FEATURES**\n",
    "\n",
    "There are pleanty of other strategies you could potentially used. You can find out more about other feature selection methods here https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Feature scaling/normalization\n",
    "\n",
    "Before making any transformation to our data, we want to make sure that the fact that different regions of the brain have different ranges will not affect our model. If a feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset. This is likely to cause issues during the model fit.\n",
    "\n",
    "There are several possible solutions to avoid having this issue. In this example, we will transform the data in such a way that the distribution of each feature will resemble a standard normal distribution (e.g. mean=0 and variance=1). Each new normalised value z is calculated by taking each data point xi, subtracting the mean x_ and then dividing it by the standard-deviation sd of the same feature:\n",
    "\n",
    "\n",
    "$$z_{x_i} = \\frac{(\\bar{x}_{feature\\, A} - x_i)}{sd_{feature\\, A}}$$\n",
    "\n",
    "\n",
    "The code below normalises each feature independently using the object StandardScaler from sklearn.\n",
    "\n",
    "First, we create the scaler object. Then, we fit the scaler parameters (mean and standard deviation) using the train set, in other words, we are calculating and storing $\\bar{x}$ and $sd$ in the object scaler. Then, we transform both the train and test set using the formula above with the stored parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train_n = scaler.transform(X_train)\n",
    "    X_test_n = scaler.transform(X_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MODEL TRAINING\n",
    "\n",
    "### 4.1. Machine learning algorithm and hyper-parameter optimization\n",
    "\n",
    "Now that we have the performance metric defined, we can specify our machine learning algorithm. In this example, we will use the popular support vector machine (SVM) as implemented by the sklearn library. You may remember from Chapter 6 that SVM allows the use of different kernels to best separate classes. Here, we will use the default linear kernel for simplicity. The use of linear kernels also make it easier to extract the correficients of the SVM model (feature importance) later on. You can find more information about different kernels at https://scikit-learn.org/stable/modules/svm.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf = LinearSVC(loss='hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, SVM relies on a hyperparameter C that regulates how much we want to avoid misclassifying each training example (see Chapter 6). The ideal method for choosing the value of C is by letting the model try several values and selecting the one with the best performance. This should be done via an additional CV inside the already defined CV, thus creating a nested CV where different values of C are fitted to the training set and tested in the validation set; the value of C with best performance is then used to fit the model to the training set as defined by the outer CV (see Chapter 2 for more details).\n",
    "\n",
    "Fortunately, sklearn has a set of useful tools to implement this. Here, we will use GridSearch, a popular choice in the brain disorders literature. You can find more about GridSearch and other methods for hyperparameter optimisation at https://scikit-learn.org/stable/modules/grid_search.html.\n",
    "\n",
    "To implement GridSearch, we first need to provide a range of possible values for C; this is our search parameter space. Next, we specify the parameters for the GridSearch. We will use stratified kfold again with 10 iterations, as with the outer CV previously defined. The final model design can be seen in Figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Hyperparameter seach space\n",
    "    param_grid = {'C': [2**-6, 2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 2**0, 2**1]}\n",
    "    \n",
    "    #Gridsearch\n",
    "    internal_cv = StratifiedKFold(n_splits=10)\n",
    "    grid_cv = GridSearchCV(estimator=clf, param_grid=param_grid, cv=internal_cv, scoring='balanced_accuracy_score', verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit our SVM model to the training data. We do this by applying the fit command to the features and labels from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    grid_result = grid_cv.fit(X_train_n, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how GridSearch works: we can see the performance for the different values of C in the validation set within the inner CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of C that yields the best performance in the validation set is shown at the top. We then define a second clasifier best_clf, where C takes the best perfomring value and fit it to the training data as defined by the outer CV. Finally, we use this model to make predictions in the test set; these are stored in y_predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_clf = grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. SVM's coefficients\n",
    "\n",
    "In addition to model performance, we are also interested in knowing which features are driving the model's predictions. In the case of the SVM with linear kernel, this feature importance is straightforward to retrieve as they are automatically stored in the parameter .coef_ from our best_clf object.\n",
    "\n",
    "We will store the coefficients for each feature for a particular CV iteration in coefficients_fold. Next, we append these values to the empty list coefficients we have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    coef_abs_value = np.abs(best_clf.coef_.squeeze())\n",
    "    cv_coefficients.append(coef_abs_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MODEL EVALUATION\n",
    "\n",
    "Once the 10 iterations of the CV are finished, we calculate the average of each chosen metric across all iterations. The result will be the final overall performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_predicted = best_clf.predict(X_test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the predicted labels, we can now estimate the performance in the test set. First, we compute the confusion matrix. From here, we estimate balanced accuracy, sensitivity, specificity and error rate. There are plenty more metrics to choose from in sklearn. For a comprehensive list see https://scikit-learn.org/stable/modules/model_evaluation.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Confusion matrix\")\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "    print(\"\")\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    test_bac = balanced_accuracy_score(y_test, y_predicted)\n",
    "    test_sens = tp / (tp + fn)\n",
    "    test_spec = tn / (tn + fp)\n",
    "\n",
    "    print(\"Balanced accuracy: %.4f \" % (test_bac))\n",
    "    print(\"Sensitivity: %.4f \" % (test_sens))\n",
    "    print(\"Specificity: %.4f \" % (test_spec))\n",
    "\n",
    "    cv_test_bac[i_fold] = test_bac\n",
    "    cv_test_sens[i_fold] = test_sens\n",
    "    cv_test_spec[i_fold] = test_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross-validation Balanced accuracy: %.4f +- %.4f\" % (cv_test_bac.mean(), cv_test_bac.std()))\n",
    "print(\"Cross-validation Sensitivity: %.4f +- %.4f\" % (cv_test_sens.mean(), cv_test_sens.std()))\n",
    "print(\"Cross-validation Specificity: %.4f +- %.4f\" % (cv_test_spec.mean(), cv_test_spec.std()))\n",
    "print(\"Cross-validation Error Rate: %.4f +- %.4f\" % (cv_error_rate.mean(), cv_error_rate.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to save your main results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving feature importance\n",
    "model_coef_df.to_csv(\"./results/\" + experiment_name + \"/feature_importance.csv\")\n",
    "\n",
    "# Saving predictions\n",
    "predictions_df.to_csv(\"./results/\" + experiment_name + \"/predictions.csv\")\n",
    "\n",
    "# Saving BAC\n",
    "bac_filename = \"./results/\" + experiment_name + \"/final_BAC.npy\"\n",
    "np.save(bac_filename, cv_test_bac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Save the model's predictions\n",
    "\n",
    "Let's save the true labels, the predicted labels and the trained model for each CV iteration. This informtion may come in handy later on for some additional analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, value in zip(test_index, y_predicted):\n",
    "    predictions_df.at[row, 'predictions'] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. POST-HOC ANALYSIS\n",
    "\n",
    "Once we have our final model, we can several additional analysis. Here, we will run the following analyses:\n",
    " - Test balanced accuracy for statistical significance via permutation testing \n",
    " - Identify the features that contributed the most for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Significance test and most informative features\n",
    "\n",
    "EXPLAIN CODE HERE IF THIS IS THE FINAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = np.mean((np.asarray(cv_coefficients)), axis=0)\n",
    "#coefficients = coefficients[(-coefficients).argsort()[:15]]\n",
    "feature_names = data.iloc[:,4:].columns\n",
    "coefficients = pd.DataFrame(data=coefficients).transpose()\n",
    "coefficients.columns=feature_names\n",
    "ax = coefficients.plot(kind='barh', figsize=(8, 10), legend=False)  # plot weights\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot above shows the most important features to classify a participant as a patient \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
